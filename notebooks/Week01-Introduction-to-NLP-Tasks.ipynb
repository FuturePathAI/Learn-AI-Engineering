{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Named Entity Recognition (NER) and Noun Phrase Extraction with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities, with their labels:\n",
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n",
      "\n",
      "Noun Phrases:\n",
      "Apple\n",
      "U.K.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Named Entity Recognition\n",
    "print(\"Named Entities, with their labels:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Noun Phrase Extraction\n",
    "print(\"\\nNoun Phrases:\")\n",
    "for np in doc.noun_chunks:\n",
    "    print(np.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Text Classification with Random Forests/xgboost and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'sci.space'])\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "print(f\"Random Forest Classifier Accuracy: {rf_clf.score(X_test, y_test)}\")\n",
    "\n",
    "# XGBoost Classifier\n",
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "print(f\"XGBoost Classifier Accuracy: {xgb_clf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Why We Don't Recommend \"Old\" NLP Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Old NLP pipelines often involved stemming, stop word removal, and bag of words models.\n",
    "\n",
    "These methods can lose important information and context, which modern techniques like\n",
    "\n",
    "TF-IDF, word embeddings, and transformers preserve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Over-fitting, Training on Test Data, and Data Leaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over-fitting occurs when a model learns the training data too well, including its noise and outliers.\n",
    "\n",
    "This results in poor generalization to new, unseen data.\n",
    "\n",
    "Training on test data or having data leaks (where information from the test set is used during training)\n",
    "\n",
    "Can lead to overly optimistic performance estimates and models that do not generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## k-Fold Stratified Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified k-Fold Cross Validation Scores: [0.96744186 0.94883721 0.98139535 0.97196262 0.98130841]\n",
      "Mean CV Score: 0.970189089328407\n"
     ]
    }
   ],
   "source": [
    "# Stratified k-Fold Cross Validation ensures that each fold of the dataset has the same proportion of classes as the original dataset.\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(rf_clf, X_tfidf, y, cv=skf)\n",
    "print(f\"Stratified k-Fold Cross Validation Scores: {scores}\")\n",
    "print(f\"Mean CV Score: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Translation and Summarization with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: Bonjour, comment Ãªtes-vous?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Translation\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "translation = translator(\"Hello, how are you?\")\n",
    "print(f\"Translation: {translation[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Your max_length is set to 142, but your input_length is only 103. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:  The quick brown fox jumps over the lazy dog . Quick brown fox jumped over the laziest dog in a series of pictures . Foxes jump over lazy dogs in a bid to keep their lazy owners happy . The foxes are often confused for each other and confused for the dog's identity .\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summary = summarizer(\"The quick brown fox jumps over the lazy dog. \" * 10)\n",
    "print(f\"Summary: {summary[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "servc_dsk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
