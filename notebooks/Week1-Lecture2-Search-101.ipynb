{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search: Retrieval & Ranking\n",
    "\n",
    "Based on [Q&A using Embeddings](https://cookbook.openai.com/examples/question_answering_using_embeddings) from OpenAI Cookbook.\n",
    "\n",
    "### Why do we even search? \n",
    "\n",
    "For every time, we need the model to use some information which it has not already seen during training. This is common when there is something new in the world, or when the model is being used in a new context e.g. your company's internal data.\n",
    "\n",
    "> GPT can learn knowledge in two ways:\n",
    ">\n",
    "> - Via model weights (i.e., fine-tune the model on a training set)\n",
    "> - Via model inputs (i.e., insert the knowledge into an input message)\n",
    "> \n",
    "> Although fine-tuning can feel like the more natural option—training on data is how GPT learned all of its other knowledge, after all—we generally do not recommend it as a way to teach the model knowledge. Fine-tuning is better suited to teaching specialized tasks or styles, and is less reliable for factual recall.\n",
    "> \n",
    "> As an analogy, model weights are like long-term memory. When you fine-tune a model, it's like studying for an exam a week away. When the exam arrives, the model may forget details, or misremember facts it never read.\n",
    "> \n",
    "> In contrast, message inputs are like short-term memory. When you insert knowledge into a message, it's like taking an exam with open notes. With notes in hand, the model is more likely to arrive at correct answers.\n",
    "\n",
    "### Retrieval Augmented Generation\n",
    "\n",
    "While we will cover this in more detail in the later chapters, it's worth mentioning here that the system can be combined with retrieved information from a database and then generate a response based on that information. This is called retrieval augmented generation.\n",
    "\n",
    "![](../assets/Retrieval%20Augemented%20Generation.gif)\n",
    "\n",
    "Following are the steps to perform retrieval augmented generation:\n",
    "\n",
    "## Retrieval\n",
    "1. Prepare search data: Prepare a dataset of documents that you want to search through.\n",
    "2. Create embeddings: Create embeddings for each document in the dataset.\n",
    "3. Prepare index: Create an index of the embeddings, this will allow you to search through the documents quickly.\n",
    "4. Search: Search through the documents using the embeddings.\n",
    "\n",
    "## Generation\n",
    "5. Generate: Use the retrieved documents to generate a response.\n",
    "\n",
    "Here, we'll quickly introduce a simplified view of the retrieval using the OpenAI API next:\n",
    "\n",
    "1. Prepare search data: You need to prepare a dataset of documents that you want to search through. This could be a list of documents, a list of paragraphs, or a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Path(\"../data/paul_graham/paul_graham_essay.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the summer of 2016, Paul Graham, the co-founder of Y Combinator, likely continued his work with the startup accelerator program and may have also been involved in various speaking engagements, writing articles, and advising startups. Unfortunately, I do not have specific details about his activities during that time period.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask(query: str, model: str = MODEL) -> str:\n",
    "    \"\"\"Return the response to a query.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "ask(\"What did Paul Graham do in Summer of 2016?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Processing: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16534"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_tokens(text: str, model: str = MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "num_tokens(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model can take a maximum of 16,385 which is less than the number of tokens in the document. We need to chunk the document into smaller pieces.\n",
    "\n",
    "Here, we'll simply split the document into approximate chunks of 1024 tokens each. This heuristic is based on empirical experiments [here](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5) by the good folks at LlamaIndex.\n",
    "\n",
    "We'd also recommend using [ChunkWiz](https://chunkviz.up.railway.app/) to build your intuition about the chunking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,\n",
       " 'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=96,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([text])\n",
    "context_text = [t.page_content for t in texts]\n",
    "len(context_text), context_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 101\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "def create_embeddings(texts: List[str], model: str = EMBEDDING_MODEL) -> List[str]:\n",
    "    \"\"\"Return the embeddings for a list of texts.\"\"\"\n",
    "    return client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "response = create_embeddings(context_text)\n",
    "print(f\"Number of documents: {len(response.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching with Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 1536)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare a list of embeddigns from the response object\n",
    "\n",
    "vectors = [d.embedding for d in response.data]\n",
    "len(vectors), len(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import models\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "qdrant_client.recreate_collection('demo',vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE))\n",
    "# Prepare the points with payload for upserting into Qdrant\n",
    "points = [\n",
    "    models.PointStruct(\n",
    "        id=i,\n",
    "        vector=vectors[i],\n",
    "        payload={\"content\":texts[i].page_content}# Assuming response.data[i] is a serializable object\n",
    "    ) for i in range(len(vectors))\n",
    "]\n",
    "qdrant_client.upload_points(collection_name='demo', points=points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = create_embeddings([\"What did Paul Graham do in Summer of 2016?\"]).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 71]\n",
      "[0.45065986325349416, 0.44844746084954556]\n"
     ]
    }
   ],
   "source": [
    "# Find the two closest elements:\n",
    "search_results = qdrant_client.search(\n",
    "    collection_name='demo',\n",
    "    query_vector=query_vector,\n",
    "    limit=2\n",
    ")\n",
    "neighbors = [hit.id for hit in search_results]\n",
    "distances = [hit.score for hit in search_results]\n",
    "print(neighbors)  # Example output: [0, 1]\n",
    "print(distances)  # Example output: [0.0, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I could not find an answer.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_with_context(query: str, context: List[str], model: str = MODEL) -> str:\n",
    "    introduction = \"\"\"Use the below writing from Paul Graham to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"\"\"\n",
    "    question = f\"\\n\\nQuestion: {query}\\n\\n\"\n",
    "    context = \"\\n\\n\".join(context)\n",
    "    return ask(introduction + context + question, model)\n",
    "\n",
    "selected_context = [context_text[neighbors[0]]]\n",
    "ask_with_context(\"What did Paul Graham do in Summer of 2016?\", selected_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the model response was more helpful than the search results. But in many cases, the search results will be more helpful than the model response. We encourage you to \"improve retrieval\" and try different search strategies to see how the model responds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
