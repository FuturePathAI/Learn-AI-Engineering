{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57303175-00e2-4dd4-b252-56c4e06e629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import chat_completion_request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976504a-7eed-423c-89c0-c9de10aa4f6f",
   "metadata": {},
   "source": [
    "# Lesson: Structured output\n",
    "\n",
    "- Structured output is data generated in a specific, pre-defined format, commonly using formats like JSON, XML, etc.\n",
    "\n",
    "- It often includes additional type information for more precise data handling, using tools like JSON Schema, Pydantic models, or Typescript objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a93b5e-4260-4c05-bb79-f0c44c47f004",
   "metadata": {},
   "source": [
    "There are several ways to get structured output from LLMs. We are going to discuss 3 in this class:\n",
    "\n",
    "1. Prompting\n",
    "2. Domain Specific Language (DSL)\n",
    "3. Model Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01449e10-1f01-44bf-b833-6b3b8b887d35",
   "metadata": {},
   "source": [
    "## 1. Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5e0f5f-2772-4df2-98ff-3af067572741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"city\": \"Paris\",\n",
      "  \"temperature\": \"20Â°C\",\n",
      "  \"condition\": \"Sunny\",\n",
      "  \"humidity\": \"50%\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example messages for weather information retrieval in JSON format\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that provides weather information. Respond in JSON format with keys: 'city', 'temperature', 'condition', and 'humidity'.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}\n",
    "]\n",
    "\n",
    "# Example call to the chat_completion_request function\n",
    "weather_response = chat_completion_request(messages=messages)\n",
    "print(weather_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35957107-fcc2-4398-bd4f-8517803c4598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sentiment>negative</sentiment>\n",
      "<score>-0.6</score>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that performs sentiment analysis. Respond in XML format with elements <sentiment> and <score>.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Analyze the sentiment of this statement: 'I love sunny days but hate the extreme heat.'\"}\n",
    "]\n",
    "\n",
    "# Example call to the chat_completion_request function\n",
    "sentiment_response = chat_completion_request(messages=messages)\n",
    "print(sentiment_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8bb5d3-aba2-40ed-aab4-13771ce229c9",
   "metadata": {},
   "source": [
    "#### Limitations of Prompting\n",
    "\n",
    "1. Dependence on Model Understanding: The accuracy of structured outputs is significantly reliant on the model's ability to correctly interpret the prompt. If the model misinterprets the prompt, it can lead to incorrect or irrelevant outputs.\n",
    "\n",
    "2. Need for Precise Prompting: Effective prompt engineering is crucial. Vague or improperly structured prompts can result in outputs that don't match the intended structure or content, necessitating skill and experience in crafting prompts.\n",
    "\n",
    "3. Inconsistency in Responses: There is often variability in the model's responses, especially for complex structured outputs or in scenarios where the model lacks sufficient training in similar tasks, leading to inconsistent and unpredictable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9064ce-d4c6-4ccd-9711-fee03d96d440",
   "metadata": {},
   "source": [
    "## 2. Domain Specific Language (DSL)\n",
    "\n",
    "This is a higher-level approach that operates at the prompt level and allows the user to specify the desired output format. \n",
    "Some popular examples of this approach are: \n",
    "\n",
    "- Microsoft's Guidance (https://github.com/guidance-ai/guidance)\n",
    "- Outlines (https://github.com/outlines-dev/outlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1dfb8d-b7be-47bf-8989-7f351ccde650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code on Colab here:\n",
    "\n",
    "# https://colab.research.google.com/drive/1PH_keLGxyDf0NJga4aPeynkPHFWXrCjb?authuser=1#scrollTo=0_UlQJmrE3sR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee42e4f-3c3c-4322-a6c6-e5f2af67dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "import torch\n",
    "\n",
    "# Define a Pydantic model for our desired JSON output\n",
    "from pydantic import BaseModel, constr\n",
    "from enum import Enum\n",
    "\n",
    "# Define an enumeration for categories\n",
    "class CategoryEnum(str, Enum):\n",
    "    technology = \"Technology\"\n",
    "    science = \"Science\"\n",
    "    art = \"Art\"\n",
    "    history = \"History\"\n",
    "    literature = \"Literature\"\n",
    "\n",
    "class Categorization(BaseModel):\n",
    "    text: constr(max_length=200)\n",
    "    category: CategoryEnum\n",
    "    confidence_score: float\n",
    "\n",
    "# Load the chosen model\n",
    "model = outlines.models.transformers(\"mistralai/Mistral-7B-v0.1\", device=\"cuda\")\n",
    "\n",
    "# Construct guided sequence generator\n",
    "generator = outlines.generate.json(model, Categorization, max_tokens=100)\n",
    "\n",
    "# Prepare the input text for categorization\n",
    "input_text = \"The discovery of the Higgs boson was a monumental step forward in particle physics.\"\n",
    "\n",
    "# Draw a sample with a random number generator for reproducibility\n",
    "rng = torch.Generator(device=\"cuda\")\n",
    "rng.manual_seed(12345)\n",
    "\n",
    "# Generate the categorization output\n",
    "sequence = generator(f\"Categorize this text: {input_text}\", rng=rng)\n",
    "print(sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0bf30-58be-4a43-a5a3-5a3404f5aa7c",
   "metadata": {},
   "source": [
    "## 3. Model Finetuning\n",
    "\n",
    "1. Task-Specific Adaptation: Fine-tuning allows the model to adapt to specific tasks or domains, enhancing its performance on targeted applications.\n",
    "\n",
    "2. Efficiency in Learning: Since the model is already pre-trained on a large dataset, fine-tuning requires less data and time to specialize the model for a new task.\n",
    "\n",
    "3. Improved Accuracy: Fine-tuned models often show improved accuracy and understanding in generating responses tailored to specific schemas or functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a09cb6-12d3-43e9-933b-d6bb83a6206a",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "1. Resource Intensive: Fine-tuning requires additional computational resources and data, which can be a limitation for smaller organizations or individual developers.\n",
    "\n",
    "2. Limited Flexibility: Adapting the model to new input or output schemas can necessitate retraining, making it less flexible to rapid changes or diverse requirements.\n",
    "\n",
    "3. Provider Specific: Fine-tuned models are often specific to the provider's ecosystem, reducing portability and limiting their application across different platforms or LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c3f75e-0478-445e-ac08-c95e92d49a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
