{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "MODEL = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "\n",
    "def chat_completion_request(messages, token_limit=100):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=token_limit\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Zero Shot Prompting\n",
    "\n",
    "- The word \"shot\" is synonymous with \"example\"\n",
    "- Zero-shot prompting can be likened to requesting someone to undertake a task they have no prior experience in, without providing any examples or guidelines.\n",
    "- Consider the scenario where you ask a friend, who has never engaged in cooking, to bake a cake. You don't provide a recipe or any demonstrations; they must depend entirely on their existing knowledge or assumptions about baking.\n",
    "- In the realm of artificial intelligence, zero-shot prompting operates in a comparable manner. Here, the AI employs its inherent knowledge, acquired during its training phase, to attempt a task for which it hasn't been explicitly prepared or shown specific examples.\n",
    "- This approach tests the AI's ability to apply its general understanding to new and unseen challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Emily Johnson [EDUCATOR]\n",
      "- Michael Smith [TECH VISIONARY]\n",
      "- John Doe [PASTRY CHEF]\n",
      "- Jane Smith [ARCHITECT]\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot example for extracting names and occupations\n",
    "zero_shot_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a model that extracts names and occupations from text.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Read the following article and list the names and occupations in the format 'First Last [OCCUPATION]': \\\n",
    "                    'In today's town news, we celebrate Emily Johnson, who has dedicated over two decades to enlightening young \\\n",
    "                    minds in our local schools, being named Educator of the Year. Meanwhile, a visionary in the tech world, Michael Smith, \\\n",
    "                    is spearheading an innovative venture in sustainable technology. Also in the news, renowned pastry chef John Doe is set \\\n",
    "                    to host a baking masterclass, while Jane Smith, known for her architectural prowess, has unveiled plans for a new city park.'\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the function with zero-shot prompting\n",
    "zero_shot_response = chat_completion_request(messages=zero_shot_messages)\n",
    "print(zero_shot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Few Shot Prompting\n",
    "\n",
    "- Few-shot prompting is akin to asking someone to perform a task with the aid of a few examples.\n",
    "- The AI is presented with a small number of examples before tackling a new task. These examples serve as a learning aid, enabling the AI to grasp the task's context and desired output.\n",
    "- This method enhances the AI's ability to adapt its pre-existing knowledge to tasks it was not explicitly trained for, using the provided examples as a learning reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily Johnson [EDUCATOR]\n",
      "Michael Smith [ENTREPRENUER]\n",
      "John Doe [PASTRY CHEF]\n",
      "Jane Smith [ARCHITECT]\n"
     ]
    }
   ],
   "source": [
    "# Few-shot example for extracting names and occupations\n",
    "few_shot_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a model that extracts names and occupations from text.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Example: Article text: 'John Doe, renowned for his exquisite pastries, has won numerous awards.' Output: John Doe [PASTRY CHEF]\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Example: Article text: 'Architect Jane Smith received acclaim for her innovative building designs.' Output: Jane Smith [ARCHITECT]\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Read the following article and list the names and occupations in the format 'First Last [OCCUPATION]': 'In today's town news, we celebrate Emily Johnson, who has dedicated over two decades to enlightening young minds in our local schools, being named Educator of the Year. Meanwhile, a visionary in the tech world, Michael Smith, is spearheading an innovative venture in sustainable technology. Also in the news, renowned pastry chef John Doe is set to host a baking masterclass, while Jane Smith, known for her architectural prowess, has unveiled plans for a new city park.'\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the function with few-shot prompting\n",
    "few_shot_response = chat_completion_request(messages=few_shot_messages)\n",
    "print(few_shot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine if the given statement is true, we need to first find the sum of odd numbers and twice the sum of even numbers in the group.\n",
      "\n",
      "Sum of odd numbers: 15 + 5 + 13 + 39 + 67 + 7 + 1 = 147\n",
      "Sum of even numbers: 32 + 82 + 48 + 26 = 188\n",
      "\n",
      "Now, we can calculate the result of subtracting twice the sum of even numbers from the sum of odd\n"
     ]
    }
   ],
   "source": [
    "# Enhanced few-shot example for a more complex reasoning task\n",
    "few_shot_messages_complex_reasoning = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a model that solves complex numerical reasoning problems.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"The sum of odd numbers in this group minus the sum of even numbers results in an odd number: 4, 8, 9, 15, 12, 2, 1. The answer is False.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"The sum of odd numbers in this group minus the sum of even numbers results in an odd number: 17, 10, 19, 4, 8, 12, 24. The answer is True.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"The sum of odd numbers in this group minus twice the sum of even numbers results in an odd number: 15, 32, 5, 13, 82, 39, 67, 7, 1, 48, 26. The answer is?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the function with few-shot prompting for complex reasoning\n",
    "few_shot_response_complex_reasoning = chat_completion_request(messages=few_shot_messages_complex_reasoning)\n",
    "print(few_shot_response_complex_reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Effective Few-Shot Prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Input Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hippopotamus is a Mammal.\n"
     ]
    }
   ],
   "source": [
    "# few-shot example with two categories\n",
    "few_shot_messages = [\n",
    "    {\"role\": \"system\",\"content\": \"You are a model that classifies animals into categories: Mammal or Bird.\"},\n",
    "    \n",
    "    {\"role\": \"user\",\"content\": \"Classify the following animal: Kangaroo.\"},\n",
    "    {\"role\": \"assistant\",\"content\": \"Kangaroo is a Mammal.\"},\n",
    "    \n",
    "    {\"role\": \"user\",\"content\": \"Classify the following animal: Elephant.\"},\n",
    "    {\"role\": \"assistant\",\"content\": \"Elephant is a Mammal.\"},\n",
    "    \n",
    "    {\"role\": \"user\",\"content\": \"Classify the following animal: Penguin.\"},\n",
    "    {\"role\": \"assistant\",\"content\": \"Penguin is a Bird.\"},\n",
    "    \n",
    "    {\"role\": \"user\",\"content\": \"Classify the following animal: Eagle.\"},\n",
    "    {\"role\": \"assistant\",\"content\": \"Eagle is a Bird.\"},\n",
    "    \n",
    "    {\"role\": \"user\",\"content\": \"Classify the following animal: Hippopotomus.\"}\n",
    "]\n",
    "\n",
    "# Call the function with revised few-shot prompting for two categories\n",
    "category = chat_completion_request(messages=few_shot_messages)\n",
    "print(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistentcy in format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est une belle journée.\n"
     ]
    }
   ],
   "source": [
    "# Few-shot example with consistent format for translation\n",
    "consistent_format_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that translates English sentences to French.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"Translate: 'Hello, how are you?'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Bonjour, comment ça va?\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"Translate: 'What is your name?'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Comment vous appelez-vous?\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"Translate: 'I am learning French.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"J'apprends le français.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"Translate: 'This is a beautiful day.'\"}\n",
    "]\n",
    "\n",
    "# Call the function with consistent format for translation\n",
    "consistent_translation = chat_completion_request(messages=consistent_format_messages)\n",
    "print(consistent_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Où se trouve la station la plus proche ?\n"
     ]
    }
   ],
   "source": [
    "# Few-shot example with inconsistent format for translation\n",
    "inconsistent_format_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that translates English sentences to French.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"How do you say in French: 'Good morning'?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Bonjour.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"Translate this: 'I like to travel.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"J'aime voyager.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"'It's raining today' in French is?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Il pleut aujourd'hui.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"How would you translate 'Where is the nearest station?' to French?\"}\n",
    "]\n",
    "\n",
    "# Call the function with inconsistent format for translation\n",
    "inconsistent_translation = chat_completion_request(messages=inconsistent_format_messages)\n",
    "print(inconsistent_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Distribution of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology\n"
     ]
    }
   ],
   "source": [
    "# Final example with true distribution of labels for categorizing news articles\n",
    "distribution_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that categorizes news articles into topics: Politics, Sports, Technology, or Entertainment.\"},\n",
    "    \n",
    "    # 40% Politics\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'Government announces new environmental policy.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Politics\"},\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'Elections results show surprising turn of events.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Politics\"},\n",
    "    \n",
    "    # 30% Sports\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'Local team wins championship after dramatic final.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sports\"},\n",
    "    \n",
    "    # 20% Technology\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'New smartphone model features the latest in AI technology.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Technology\"},\n",
    "    \n",
    "    # 10% Entertainment\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'Famous actor stars in a new blockbuster movie.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Entertainment\"},\n",
    "    \n",
    "    # New article to categorize\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'Breakthrough in Renewable Energy Technology Unveiled at Global Conference.'\"}\n",
    "]\n",
    "\n",
    "# Call the function with final distribution of labels for categorizing news articles\n",
    "categorization = chat_completion_request(messages=distribution_messages)\n",
    "print(categorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Lost in the Middle\n",
    "\n",
    "- is a phenomenon where the model pays more attention to the beginning and end of a text, while important details in the middle may be overlooked or given less emphasis.\n",
    "- typically occurs in long texts or complex prompts, leading to incomplete or inaccurate processing of the full content\n",
    "- somewhat similar to a person skimming through a long article and mainly remembering the introduction and conclusion, while missing key points in the middle\n",
    "- To counter this, breaking down tasks into smaller and add more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Day 1: Tokyo\n",
      "#### Weather: March 1, 2024 - Partly cloudy, 15°C / 59°F\n",
      "\n",
      "- **Morning:** Begin the day with a visit to Senso-ji Temple in Asakusa, followed by shopping at Nakamise Street for traditional handicrafts. Enjoy a relaxing boat ride along the Sumida River.\n",
      "- **Lunch:** Head to Tsurutontan Udon for allergy-friendly udon noodles with various toppings. They offer gluten-free options.\n",
      "- **Afternoon:** Explore the kid-friendly Ueno Zoo and unwind at Ueno Park. Try traditional Japanese tea at the nearby Kan'eiji Temple.\n",
      "- **Dinner:** Dine at Kyushu Jangara Ramen, a hidden gem known for its customizable ramen bowls. They offer vegetarian and vegan options.\n",
      "- **Cultural Event:** Participate in a Taiko drumming class at Asakusa Culture Tourist Information Center.\n",
      "\n",
      "#### Eco-Friendly Accommodation in Tokyo:\n",
      "- **Stay:** Book a stay at the Wired Hotel in Asakusa, which emphasizes sustainability and offers modern comforts with a touch of traditional Japanese design.\n",
      "\n",
      "#### Transportation: \n",
      "- Use the Tokyo Metro for getting around the city. Consider hiring bicycles for a more eco-friendly option for shorter distances.\n",
      "\n",
      "### Day 2: Tokyo\n",
      "#### Weather: March 2, 2024 - Sunny, 17°C / 63°F\n",
      "\n",
      "- **Morning:** Visit the Edo-Tokyo Museum for a glimpse into Tokyo's history. Explore the trendy neighborhoods of Harajuku and Omotesando for unique shopping experiences.\n",
      "- **Lunch:** Try allergy-friendly sushi rolls at Sushiya no Nohachi, a charming sushi bar known for its fresh ingredients and accommodating staff.\n",
      "- **Afternoon:** Head to Shinjuku Gyoen National Garden for a peaceful stroll amongst cherry blossoms. Visit Meiji Shrine for a serene cultural experience.\n",
      "- **Dinner:** Enjoy traditional Kaiseki cuisine at Kagurazaka Ishikawa, serving seasonal dishes in a cozy setting.\n",
      "- **Relaxation Spot:** Unwind at an onsen at Odaiba Oedo Onsen Monogatari for a relaxing soak in natural hot springs.\n",
      "\n",
      "#### Transportation:\n",
      "- Purchase a JR Pass for the upcoming journey to Kyoto the next day.\n",
      "\n",
      "### Day 3: Kyoto\n",
      "#### Weather: March 3, 2024 - Cloudy, 14°C / 57°F\n",
      "\n",
      "- **Morning:** Arrive in Kyoto and have a leisurely breakfast at %Arabica Kyoto, known for its allergy-friendly coffee and pastries.\n",
      "- **Morning:** Explore the Arashiyama Bamboo Grove and enjoy a scenic boat ride on the Hozu River.\n",
      "- **Lunch:** Taste traditional Kyoto Obanzai dishes at Menami, a cozy restaurant offering allergen-free options.\n",
      "- **Afternoon:** Visit Kinkaku-ji (Golden Pavilion) and the nearby Ryoan-ji Zen garden for a serene cultural experience.\n",
      "- **Dinner:** Dine at Ganko Sushi for fresh and allergy-friendly sushi made with locally sourced ingredients.\n",
      "\n",
      "#### Eco-Friendly Accommodation in Kyoto:\n",
      "- **Stay:** Book a stay at Guesthouse Yululu in Gion, an eco-conscious guesthouse located in the heart of Kyoto's historic district.\n",
      "\n",
      "#### Transportation:\n",
      "- Use Kyoto's efficient bus system to explore the city's various attractions.\n",
      "\n",
      "### Day 4: Kyoto\n",
      "#### Weather: March 4, 2024 - Rainy, 13°C / 55°F\n",
      "\n",
      "- **Morning:** Explore Fushimi Inari Taisha and hike through the iconic torii gates. Get lunch from food stalls offering allergen-free grilled skewers.\n",
      "- **Afternoon:** Discover the historic district of Higashiyama and visit Kodai-ji Temple. Participate in a family-friendly tea ceremony at Camellia Flower Shop.\n",
      "- **Dinner:** Indulge in allergy-friendly yuba dishes at Yuba Jin Soba Shijo Kitanotenmangu, known for its soy-based cuisine.\n",
      "\n",
      "#### Cultural Event:\n",
      "- Participate in a kimono dressing workshop at Yumeyakata to learn about traditional Japanese attire.\n",
      "\n",
      "### Day 5: Osaka\n",
      "#### Weather: March 5, 2024 - Clear skies, 18°C / 64°F\n",
      "\n",
      "- **Morning:** Travel to Osaka and visit the bustling Kuromon Ichiba Market for a taste of local flavors. Try grilled takoyaki for breakfast.\n",
      "- **Lunch:** Head to Mizuno for allergy-friendly okonomiyaki, a savory pancake filled with various ingredients, at this hidden gem in Dotonbori.\n",
      "- **Afternoon:** Explore the historic Osaka Castle and its surrounding park. Rent bicycles and cycle along the nearby Yodo River.\n",
      "- **Dinner:** Have a luxurious vegan dinner at 222 Veggie Vegan\n"
     ]
    }
   ],
   "source": [
    "# Example showing an overloaded task for creating a detailed travel itinerary\n",
    "overloaded_itinerary_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that helps plan travel itineraries.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Create a week-long travel itinerary for a trip to Japan, starting from March 1, 2024. \"\n",
    "        \"Include a day-by-day schedule covering Tokyo, Kyoto, and Osaka. \"\n",
    "        \"For each city, recommend unique, non-touristy restaurants and hidden gems, \"\n",
    "        \"specifying dishes that are both traditional and allergy-friendly. \"\n",
    "        \"Include cultural events happening during the week, particularly those that \"\n",
    "        \"allow for active participation. Suggest accommodations that are eco-friendly, \"\n",
    "        \"budget-conscious, and offer traditional experiences. Detail transportation options \"\n",
    "        \"between cities and within each city, prioritizing eco-friendly choices and cost-effectiveness. \"\n",
    "        \"Each day should have outdoor activities, historical sites, shopping in local markets, \"\n",
    "        \"and relaxation spots like onsens. Also, provide weather advice for each day, language tips, \"\n",
    "        \"and essential cultural etiquette to follow. Ensure the plan caters to a family with young children, \"\n",
    "        \"considering accessibility and family-friendly activities.\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "# Call the function with the overloaded itinerary task\n",
    "overloaded_itinerary_response = chat_completion_request(messages=overloaded_itinerary_request, token_limit=1000)\n",
    "print(overloaded_itinerary_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction:\n",
      "Climate change is having a significant impact on global agriculture, posing serious threats to food production and food security worldwide. As temperatures rise, weather patterns become more unpredictable, and extreme weather events become more frequent, farmers are facing new challenges in growing crops and raising livestock. Understanding the effects of climate change on agriculture is crucial in developing sustainable solutions to mitigate its impact.\n",
      "\n",
      "Effects on different types of crops:\n",
      "Different types of crops are affected by climate change in various ways. For example, rising temperatures and changing precipitation patterns have led to prolonged droughts in some regions, reducing crop yields and quality. On the other hand, increased frequency of extreme weather events such as floods and storms can damage crops and result in significant economic losses. Additionally, changing temperatures can also disrupt plant development and flowering cycles, impacting the timing of planting and harvesting.\n",
      "\n",
      "Staple crops like wheat, maize, and rice are particularly vulnerable to climate change, as they are sensitive to extreme temperatures and water stress. Fruits and vegetables are also at risk, as they require specific temperature and moisture conditions for optimal growth. Livestock farming is affected as well, as heat stress can reduce productivity and increase mortality rates among animals.\n",
      "\n",
      "How farmers are adapting:\n",
      "In response to the challenges posed by climate change, farmers are implementing various adaptation strategies to safeguard their crops and livelihoods. These include practicing diversified cropping systems, optimizing water use through irrigation techniques, adopting conservation agriculture practices, and planting heat-tolerant and drought-resistant crop varieties. Farmers are also using weather data and predictive analytics to make informed decisions about planting and harvesting times, as well as implementing pest and disease management strategies.\n",
      "\n",
      "Role of technology in addressing these challenges:\n",
      "Technology plays a crucial role in helping farmers adapt to the impacts of climate change on agriculture. Precision agriculture tools, such as drones and satellite imaging, allow farmers to monitor crop health, soil conditions, and water usage more efficiently. Climate-smart technologies like drip irrigation systems, rainwater harvesting, and solar-powered pumps help in conserving water resources and reducing carbon emissions. Biotechnology and genetic engineering are also being used to develop crop varieties that are more resilient to extreme weather conditions and pests.\n",
      "\n",
      "Conclusion:\n",
      "Climate change poses a significant threat to global agriculture, jeopardizing food production and food security for millions of people around the world. The effects of climate change on agriculture are far-reaching, impacting various types of crops and livestock systems. However, with the right adaptation strategies and the deployment of innovative technologies, farmers can mitigate some of these risks and build more resilient agricultural systems. It is essential for policymakers, researchers, and the agricultural community to work together to develop sustainable solutions that address the challenges of climate change and ensure a stable food supply for future generations.\n"
     ]
    }
   ],
   "source": [
    "# Bad Example: Asking for an Entire Article in One Prompt\n",
    "full_article_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that helps write articles.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Write a complete article about the impact of climate change on global agriculture. \"\n",
    "        \"Include an introduction, the effects on different types of crops, how farmers are adapting, \"\n",
    "        \"the role of technology in addressing these challenges, and a conclusion.\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "# Call the function with the full article request\n",
    "full_article_response = chat_completion_request(messages=full_article_request, token_limit=1000)\n",
    "print(full_article_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I. Introduction\n",
      "    A. Brief overview of climate change and agriculture\n",
      "    B. Importance of agriculture in feeding the global population\n",
      "\n",
      "II. Climate Change's Impact on Agriculture\n",
      "    A. Shifts in temperature and precipitation patterns\n",
      "        1. Effects on crop growth and productivity\n",
      "        2. Increased frequency of extreme weather events\n",
      "    B. Changes in pests and diseases\n",
      "        1. Spread of pests to new regions\n",
      "        2. Impact on crop yields and quality\n",
      "Introduction:\n",
      "\n",
      "In the fast-paced world of fashion and beauty, staying ahead of the game is crucial for both industry professionals and style enthusiasts alike. From runway trends to skincare innovations, keeping up with the latest developments is essential to setting yourself apart in this ever-evolving landscape. In this article, we'll explore some of the hottest fashion and beauty trends of the season, offering insights and tips to help you elevate your style game and feel confident in your skin. Let's dive in and discover what's trending\n",
      "I'll be happy to help! Please go ahead and provide the text of the article for me to review.\n"
     ]
    }
   ],
   "source": [
    "# Good example: Breaking down the article writing process\n",
    "\n",
    "# Step 1: Asking for an outline\n",
    "outline_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that helps write article outlines.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Create an outline for an article about the impact of climate change on global agriculture.\"}\n",
    "]\n",
    "\n",
    "# Call the function for the outline\n",
    "outline_response = chat_completion_request(messages=outline_request)\n",
    "print(outline_response)\n",
    "\n",
    "# Assuming the outline is received, proceed with writing sections\n",
    "\n",
    "# Step 2: Writing individual sections (Example: Writing the introduction)\n",
    "introduction_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that helps write article sections.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write the introduction for the article based on the provided outline.\"}\n",
    "]\n",
    "\n",
    "# Call the function for the introduction section\n",
    "introduction_response = chat_completion_request(messages=introduction_request)\n",
    "print(introduction_response)\n",
    "\n",
    "# Similarly, continue with other sections...\n",
    "\n",
    "# Step 3: Refining the entire article\n",
    "# This step is done after all sections are written\n",
    "article_refinement_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that helps refine articles.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Here is the complete article [insert the written article]. Can you suggest improvements?\"}\n",
    "]\n",
    "\n",
    "# Call the function for refining the article\n",
    "article_refinement_response = chat_completion_request(messages=article_refinement_request,token_limit=1000)\n",
    "print(article_refinement_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Markdown Tricks\n",
    "\n",
    "- Markdown Basics: Markdown is a lightweight markup language that allows for easy text formatting using plain text syntax. It's widely used for creating formatted text on the web.\n",
    "\n",
    "- Simplicity and Readability: One of the key features of Markdown is its simplicity. The syntax is designed to be readable and straightforward, making it easy to write and read even in its raw form.\n",
    "\n",
    "## In short:\n",
    "- Use Markdown to create a clear and concise structure for your prompts.\n",
    "- Use Markdown to highlight important information in your prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Here are the markdown indicators for ChatGPT :\n",
    "#     1. # Heading 1\n",
    "#     2. ## Heading 2\n",
    "#     3. ### Heading 3\n",
    "#     4. *italic text*, **bold text**, __underlined text__\n",
    "#     5. [Hyperlink text](url)\n",
    "#     6. ![Image name](image url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the breakdown of the tasks with their respective deadlines and key points to consider:\n",
      "\n",
      "### Tasks List:\n",
      "1. **Research Task**:\n",
      "   - **Deadline:** Next Monday\n",
      "   - **Description:** Compile a list of the top 5 AI trends in 2024.\n",
      "   - **Priority:** High\n",
      "   - **Note:** Ensure the accuracy and relevance of the information gathered.\n",
      "\n",
      "2. **Writing Task**:\n",
      "   - **Deadline:** Wednesday\n",
      "   - **Description:** Write a summary of each trend, focusing on their impact on healthcare.\n",
      "   - **Priority:** High\n",
      "   - **Note:** Accuracy of data is essential. Address the healthcare impact prominently.\n",
      "\n",
      "3. **Presentation Task**:\n",
      "   - **Deadline:** Friday\n",
      "   - **Description:** Create a slide deck for the AI trends with engaging and informative visuals.\n",
      "   - **Priority:** High\n",
      "   - **Note:** Ensure the presentation is clear, engaging, and provides valuable insights.\n",
      "\n",
      "4. **Feedback Session**:\n",
      "   - **Deadline:** Next Friday\n",
      "   - **Description:** Organize a team meeting to review and discuss the slide deck.\n",
      "   - **Priority:** High\n",
      "   - **Note:** Invite all project members for thorough feedback on the presentation.\n",
      "\n",
      "### Key Points to Prioritize:\n",
      "- **Accuracy:** Ensure the information is accurate in both research and writing tasks.\n",
      "- **Relevance:** Focus on the impact of AI trends on healthcare in the writing task.\n",
      "- **Engagement:** Make the presentation visually appealing and engaging for the audience.\n",
      "- **Collaboration:** Facilitate communication and feedback in the team meeting for continuous improvement.\n",
      "\n",
      "Please let me know if you need any further assistance or details on these tasks.\n"
     ]
    }
   ],
   "source": [
    "# Example of multi-task instruction with emphasis and list organization using Markdown\n",
    "multi_task_instruction_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that understands and organizes tasks using Markdown.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Here's a list of tasks I need you to perform. Please pay **special attention** to the deadlines:\\n\\n\"\n",
    "        \"* **Research Task**: Compile a list of the top 5 AI trends in 2024. *Due: Next Monday*\\n\"\n",
    "        \"* **Writing Task**: Write a summary of each trend, emphasizing their impact on healthcare. **Ensure accuracy** in data. *Due: Wednesday*\\n\"\n",
    "        \"* **Presentation Task**: Create a slide deck for the AI trends with appropriate visuals. Remember to make it **engaging and informative**. *Due: Friday*\\n\"\n",
    "        \"* **Feedback Session**: Organize a team meeting to discuss the slide deck. Ensure to invite all project members. *Due: Next Friday*\\n\\n\"\n",
    "        \"__Note__: The quality of research and clarity in the presentation are crucial. Please prioritize these aspects.\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "# Call the function to organize and emphasize the tasks\n",
    "multi_task_instruction_response = chat_completion_request(messages=multi_task_instruction_request, token_limit=1000)\n",
    "print(multi_task_instruction_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"selected_function\": \"get_order_status\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Example of selecting a function with Markdown organization and JSON response\n",
    "function_selection_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that selects functions and formats responses in JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Below is a list of functions. Read the descriptions and select the most relevant function based on the user query. \"\n",
    "        \"Respond in JSON format. If the necessary information is not available, return an empty JSON response.\\n\\n\"\n",
    "        \n",
    "        \"### Available Functions:\\n\\n\"\n",
    "        \n",
    "        \"1. **get_order_status**: Retrieve order details including status and tracking information. \"\n",
    "        \"*Required Input*: Order ID, Email, or Phone Number.\\n\\n\"\n",
    "        \n",
    "        \"2. **manage_order_change_request**: Handle modifications to an existing order. \"\n",
    "        \"*Required Input*: Order ID, Email, or Phone Number.\\n\\n\"\n",
    "        \n",
    "        \"3. **get_public_info**: Access store-specific public information like policies and promotions. \"\n",
    "        \"*Required Input*: None\\n\\n\"\n",
    "        \n",
    "        \"### User Query: 'I want to know when my order will arrive.'\\n\\n\"\n",
    "        \"Based on the query, select the appropriate function and provide a JSON response.\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "# Call the function to select and format the response\n",
    "function_selection_response = chat_completion_request(messages=function_selection_request)\n",
    "print(function_selection_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Prompt Order Sensitivity\n",
    "\n",
    "- **Understand Prompt Order Sensitivity**: Recognize that the order in which examples are presented in a few-shot prompt can significantly impact an AI model's performance. Different orders can lead to varying levels of understanding and accuracy.\n",
    "\n",
    "- **Experiment with Different Orders**: To identify the most effective prompt order, experiment with rearranging your examples. This trial-and-error approach can reveal which sequence yields the best responses from the model.\n",
    "\n",
    "- **Start with Simple to Complex Examples**: When uncertain, a good rule of thumb is to arrange examples from the simplest to the most complex. This can help the model build its understanding progressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Chain of Thought\n",
    "\n",
    "- **Understand Chain of Thought**: Recognize that guiding the model through a step-by-step reasoning process can significantly enhance its performance. This method helps the model break down complex tasks into manageable steps.\n",
    " \n",
    "- **Encourage Detailed Reasoning**: To leverage the chain of thought approach, prompt the model to explain its reasoning at each step. This can lead to more accurate and reliable outputs.\n",
    " \n",
    "- **Use Clear and Logical Steps**: When uncertain, a good rule of thumb is to structure the reasoning process in clear and logical steps. This helps the model follow a coherent path to the solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"response\": {\n",
      "        \"selected_function\": \"get_order_status\",\n",
      "        \"response_format\": \"JSON\"\n",
      "    },\n",
      "    \"reason\": \"The user query indicates a request for order delivery information, which aligns with the function 'get_order_status' that retrieves order details including status and tracking information.\",\n",
      "    \"function_details\": {\n",
      "        \"function_name\": \"get_order_status\",\n",
      "        \"description\": \"Retrieve order details including status and tracking information.\",\n",
      "        \"required_input\": \"Order\n"
     ]
    }
   ],
   "source": [
    "# Example of chain of thought approach with Markdown organization and JSON response\n",
    "chain_of_thought_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that uses a chain of thought approach to select functions and format responses in JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Below is a list of functions. Read the descriptions and select the most relevant function based on the user query. \"\n",
    "        \"Respond in JSON format. If the necessary information is not available, return an empty JSON response.\\n\\n\"\n",
    "        \"Follow the below chain of thought approach to select the appropriate function and provide a JSON response.\\n\\n\"\n",
    "        \"Step 1: Identify the user's intent.\\n\"\n",
    "        \"Step 2: Determine which function provides details of the user query .\\n\"\n",
    "        \"Step 3: Select the appropriate function that matches the user's intent.\\n\"\n",
    "        \"Step 4: Format the response in JSON.\"\n",
    "\n",
    "        \"### Available Functions:\\n\\n\"\n",
    "        \n",
    "        \"1. **get_order_status**: Retrieve order details including status and tracking information. \"\n",
    "        \"*Required Input*: Order ID, Email, or Phone Number.\\n\\n\"\n",
    "        \n",
    "        \"2. **manage_order_change_request**: Handle modifications to an existing order. \"\n",
    "        \"*Required Input*: Order ID, Email, or Phone Number.\\n\\n\"\n",
    "        \n",
    "        \"3. **get_public_info**: Access store-specific public information like policies and promotions. \"\n",
    "        \"*Required Input*: None\\n\\n\"\n",
    "        \n",
    "        \"### User Query: 'I want to know when my order will arrive.'\\n\\n\"\n",
    "        \"Now break down the query step-by-step to select the appropriate function and provide a JSON response.\\n\\n\"\n",
    "\n",
    "        \"Also provide a reason and your approach towards the solution in a tag named reason in the JSON response.\\n\\n\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "# Call the function to select and format the response\n",
    "chain_of_thought_response = chat_completion_request(messages=chain_of_thought_request)\n",
    "print(chain_of_thought_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Lesson 6: LogProbs\n",
    "\n",
    "Logprobs provide the log probabilities of each output token, indicating the model's confidence in its predictions. They help in understanding the likelihood of different tokens and exploring alternative responses.\n",
    "\n",
    "When logprobs is enabled, the API returns the log probabilities of each output token, along with a limited number of the most likely tokens at each token position and their log probabilities. The relevant request parameters are:\n",
    "\n",
    "- **logprobs**: Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message. This option is currently not available on the gpt-4-vision-preview model.\n",
    "- **top_logprobs**: An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n",
    "\n",
    "Log probabilities of output tokens indicate the likelihood of each token occurring in the sequence given the context. To simplify, a logprob is log(p), where p = probability of a token occurring at a specific position based on the previous tokens in the context. Some key points about logprobs:\n",
    " \n",
    "- Higher log probabilities suggest a higher likelihood of the token in that context. This allows users to gauge the model's confidence in its output or explore alternative responses the model considered.\n",
    "- Logprob can be any negative number or 0.0. 0.0 corresponds to 100% probability.\n",
    "- Logprobs allow us to compute the joint probability of a sequence as the sum of the logprobs of the individual tokens. This is useful for scoring and ranking model outputs. Another common approach is to take the average per-token logprob of a sentence to choose the best generation.\n",
    "- We can examine the logprobs assigned to different candidate tokens to understand what options the model considered plausible or implausible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin with a prompt that presents the model with four categories: Technology, Politics, Sports, and Arts. The model is then tasked with classifying articles into these categories based solely on their headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_PROMPT = \"\"\"You will be given a headline of a news article.\n",
    "Classify the article into one of the following categories: Technology, Politics, Sports, and Art.\n",
    "Return only the name of the category, and nothing else.\n",
    "MAKE SURE your output is one of the four categories stated.\n",
    "Article headline: {headline}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at three sample headlines, and first begin with a standard Chat Completions output, without logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [\n",
    "    \"Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\",\n",
    "    \"Local Mayor Launches Initiative to Enhance Urban Public Transport.\",\n",
    "    \"Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n",
      "Category: Technology\n",
      "\n",
      "\n",
      "Headline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\n",
      "Category: Politics\n",
      "\n",
      "\n",
      "Headline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\n",
      "Category: Art\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for headline in headlines:\n",
    "    print(f\"\\nHeadline: {headline}\")\n",
    "    API_RESPONSE = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n",
    "        model=\"gpt-4\",\n",
    "    )\n",
    "    print(f\"Category: {API_RESPONSE.choices[0].message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to print the log probability as a linear probability:\n",
    "1. Access the log probability value using logprob.logprob. Log probabilities are typically negative numbers, where a higher (less negative) value indicates a higher probability.\n",
    "2. Convert the log probability to a linear probability using the np.exp function from the NumPy library. The exponential function is the inverse of the natural logarithm, so if logprob.logprob is log(p), then np.exp(logprob.logprob) gives p.\n",
    "3. Convert the linear probability to a percentage by multiplying it by 100.\n",
    "4. Round the percentage to two decimal places for readability using np.round(..., 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Technology, <span style='color: darkorange'>logprobs:</span> -2.220075e-06, <span style='color: magenta'>linear probability:</span> 100.0%<br><span style='color: cyan'>Output token 2:</span> Techn, <span style='color: darkorange'>logprobs:</span> -13.829107, <span style='color: magenta'>linear probability:</span> 0.0%<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Headline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Politics, <span style='color: darkorange'>logprobs:</span> -3.1737043e-06, <span style='color: magenta'>linear probability:</span> 100.0%<br><span style='color: cyan'>Output token 2:</span> Technology, <span style='color: darkorange'>logprobs:</span> -13.269092, <span style='color: magenta'>linear probability:</span> 0.0%<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Headline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Art, <span style='color: darkorange'>logprobs:</span> -0.0016592321, <span style='color: magenta'>linear probability:</span> 99.83%<br><span style='color: cyan'>Output token 2:</span> Sports, <span style='color: darkorange'>logprobs:</span> -6.4027405, <span style='color: magenta'>linear probability:</span> 0.17%<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for headline in headlines:\n",
    "    print(f\"\\nHeadline: {headline}\")\n",
    "    API_RESPONSE = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n",
    "        model=\"gpt-4\",\n",
    "        logprobs=True,\n",
    "        top_logprobs=2,\n",
    "    )\n",
    "    top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n",
    "    html_content = \"\"\n",
    "    for i, logprob in enumerate(top_two_logprobs, start=1):\n",
    "        html_content += (\n",
    "            f\"<span style='color: cyan'>Output token {i}:</span> {logprob.token}, \"\n",
    "            f\"<span style='color: darkorange'>logprobs:</span> {logprob.logprob}, \"\n",
    "            f\"<span style='color: magenta'>linear probability:</span> {np.round(np.exp(logprob.logprob)*100,2)}%<br>\"\n",
    "        )\n",
    "    display(HTML(html_content))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "servc_dsk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
