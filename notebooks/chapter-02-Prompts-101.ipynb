{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Chat Completion API Call\n",
    "\n",
    "A chat completion API call parameters,\n",
    "**Required**\n",
    "- `model`: the name of the model you want to use (e.g., `gpt-3.5-turbo`, `gpt-4`, `gpt-3.5-turbo-16k-1106`)\n",
    "- `messages`: a list of message objects, where each object has two required fields:\n",
    "    - `role`: the role of the messenger (either `system`, `user`, `assistant` or `tool`)\n",
    "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
    "\n",
    "Messages can also contain an optional `name` field, which give the messenger a name. E.g., `example-user`, `Alice`, `BlackbeardBot`. Names may not contain spaces.\n",
    "\n",
    "**Optional**\n",
    "- `frequency_penalty`: Penalizes tokens based on their frequency, reducing repetition.\n",
    "- `logit_bias`: Modifies likelihood of specified tokens with bias values.\n",
    "- `logprobs`: Returns log probabilities of output tokens if true.\n",
    "- `top_logprobs`: Specifies the number of most likely tokens to return at each position.\n",
    "- `max_tokens`: Sets the maximum number of generated tokens in chat completion.\n",
    "- `n`: Generates a specified number of chat completion choices for each input.\n",
    "- `presence_penalty`: Penalizes new tokens based on their presence in the text.\n",
    "- `response_format`: Specifies the output format, e.g., JSON mode.\n",
    "- `seed`: Ensures deterministic sampling with a specified seed.\n",
    "- `stop`: Specifies up to 4 sequences where the API should stop generating tokens.\n",
    "- `stream`: Sends partial message deltas as tokens become available.\n",
    "- `temperature`: Sets the sampling temperature between 0 and 2.\n",
    "- `top_p`: Uses nucleus sampling; considers tokens with top_p probability mass.\n",
    "- `tools`: Lists functions the model may call.\n",
    "- `tool_choice`: Controls the model's function calls (none/auto/function).\n",
    "- `user`: Unique identifier for end-user monitoring and abuse detection.\n",
    "\n",
    "\n",
    "As of January 2024, you can also optionally submit a list of `functions` that tell GPT whether it can generate JSON to feed into a function. For details, see the [documentation](https://platform.openai.com/docs/guides/function-calling), [API reference](https://platform.openai.com/docs/api-reference/chat), or the Cookbook guide [How to call functions with chat models](How_to_call_functions_with_chat_models.ipynb).\n",
    "\n",
    "Typically, a conversation will start with a system message that tells the assistant how to behave, followed by alternating user and assistant messages, but you are not required to follow this format.\n",
    "\n",
    "Let's look at an example chat API calls to see how the chat format works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Essentials\n",
    "\n",
    "### System Messages\n",
    "\n",
    "Unlike most other open source models, OpenAI models -- specially those after 0613, pay attention to the system messages. You can use system messages to set the overall tone for the conversation e.g. `role: system, content: \"You're a friendly assistant.\"` or `role: system, content: \"You're a friendly teaching assistant who is talking to a fifth grader with English as a second language.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In May, Nagpur typically experiences hot and dry weather with temperatures ranging from 30°C to 45°C. It is advisable to stay hydrated and seek shade during the peak afternoon hours.\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to be brief\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise, crisp assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Describe the weather in Nagpur in May\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
    "\n",
    "One way to show the model what you want is with faked example messages.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sudden change in direction means we don't have time to do everything for the client's project.\n"
     ]
    }
   ],
   "source": [
    "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful, pattern-following assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Help me translate the following corporate jargon into plain English.\",\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Things working well together will increase revenue.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
