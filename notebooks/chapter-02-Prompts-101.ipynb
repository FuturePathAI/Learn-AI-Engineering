{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Chat Completion API Call\n",
    "\n",
    "[OpenAI Playground](https://platform.openai.com/playground/chat) is a great place to experiment with the Chat Completion API. You can try out different models, tweak parameters, and see how the model responds to different prompts.\n",
    "\n",
    "A chat completion API call parameters,\n",
    "**Required**\n",
    "- `model`: the name of the model you want to use (e.g., `gpt-3.5-turbo`, `gpt-4`, `gpt-3.5-turbo-16k-1106`)\n",
    "- `messages`: a list of message objects, where each object has two required fields:\n",
    "    - `role`: the role of the messenger (either `system`, `user`, `assistant` or `tool`)\n",
    "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
    "\n",
    "Messages can also contain an optional `name` field, which give the messenger a name. E.g., `example-user`, `Alice`, `BlackbeardBot`. Names may not contain spaces.\n",
    "\n",
    "**Optional**\n",
    "- `frequency_penalty`: Penalizes tokens based on their frequency, reducing repetition.\n",
    "- `presence_penalty`: Penalizes new tokens based on their presence in the text.\n",
    "- `logit_bias`: Modifies likelihood of specified tokens with bias values.\n",
    "- `logprobs`: Returns log probabilities of output tokens if true.\n",
    "- `top_logprobs`: Specifies the number of most likely tokens to return at each position.\n",
    "- `max_tokens`: Sets the maximum number of generated tokens in chat completion.\n",
    "- `n`: Generates a specified number of chat completion choices for each input.\n",
    "- `seed`: Ensures deterministic sampling with a specified seed.\n",
    "- `stop`: Specifies up to 4 sequences where the API should stop generating tokens.\n",
    "- `stream`: Sends partial message deltas as tokens become available.\n",
    "- `temperature`: Sets the sampling temperature between 0 and 2.\n",
    "- `top_p`: Uses nucleus sampling; considers tokens with top_p probability mass.\n",
    "    - Reference [medium blog](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277)\n",
    "\n",
    "\n",
    "**Function Calling and Tool Usage**\n",
    "- `response_format`: Specifies the output format, e.g., JSON mode.\n",
    "- `tools`: Lists functions the model may call.\n",
    "- `tool_choice`: Controls the model's function calls (none/auto/function).\n",
    "- `user`: Unique identifier for end-user monitoring and abuse detection.\n",
    "\n",
    "As of January 2024, you can also optionally submit a list of `functions` that tell GPT whether it can generate JSON to feed into a function. For details, see the [documentation](https://platform.openai.com/docs/guides/function-calling), [API reference](https://platform.openai.com/docs/api-reference/chat), or the Cookbook guide [How to call functions with chat models](How_to_call_functions_with_chat_models.ipynb).\n",
    "\n",
    "Typically, a conversation will start with a system message that tells the assistant how to behave, followed by alternating user and assistant messages, but you are not required to follow this format.\n",
    "\n",
    "Let's look at an example chat API calls to see how the chat format works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Essentials\n",
    "\n",
    "### System Messages\n",
    "\n",
    "Unlike most other open source models, OpenAI models -- specially those after 0613, pay attention to the system messages. You can use system messages to set the overall tone for the conversation e.g. `role: system, content: \"You're a friendly assistant.\"` or `role: system, content: \"You're a friendly teaching assistant who is talking to a fifth grader with English as a second language.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In May, Nagpur typically experiences hot and dry weather with temperatures ranging from 30°C to 45°C. It is advisable to stay hydrated and seek shade during the peak afternoon hours.\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to be brief\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise, crisp assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Describe the weather in Nagpur in May\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
    "\n",
    "One way to show the model what you want is with faked example messages.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sudden change in direction means we don't have time to do everything for the client's project.\n"
     ]
    }
   ],
   "source": [
    "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful, pattern-following assistant who speaks plain English.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Help me translate the following corporate jargon into plain English.\",\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Things working well together will increase revenue.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
