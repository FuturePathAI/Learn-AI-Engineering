{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83fe9c-525f-413c-b809-a781194579b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from learn_ai.scripts.utils import chat_completion_request\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5358731",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY_FUTUREPATH_ML\")\n",
    "os.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b37fe-14da-4f44-b8d0-4d3902bc3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('data/cot.png')\n",
    "display(img)\n",
    "\n",
    "# image source https://arxiv.org/abs/2201.11903"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64621d31-8832-4dbb-8ac2-470ba992e955",
   "metadata": {},
   "source": [
    "# Lesson: Chain of Thought Prompting\n",
    "\n",
    "- chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps\n",
    "- combine it with few-shot prompting to get better results on more complex tasks that require reasoning\n",
    "- CoT is ideal when your task involves complex reasoning that require arithmetic, commonsense, and symbolic reasoning; where the model needs to understand and follow intermediate steps to arrive at the correct answer.\n",
    "\n",
    "#### Difference between Few-Shot prompting and Chain-of-Thought?\n",
    "- Few-shot prompting is when you give a few examples so the language model can understand want it should do.\n",
    "\n",
    "- Chain-of-Thought prompting is about showing the step-by-step thinking from start to finish, which helps with “reasoning” and getting more detailed answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9d983-5d47-4ba4-a50f-642736a88d9b",
   "metadata": {},
   "source": [
    "## Zero-shot CoT: Let's think step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46efbf6-10fc-4c70-ae11-9c03c688c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('data/zero-cot.png')\n",
    "display(img)\n",
    "\n",
    "# image source https://arxiv.org/abs/2205.11916"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a8ef9-29d5-4115-8515-9248bffceca2",
   "metadata": {},
   "source": [
    "## Auto CoT\n",
    "\n",
    "https://arxiv.org/abs/2210.03493\n",
    "\n",
    "https://github.com/amazon-science/auto-cot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac67cf8-9a33-45f4-aaaa-a2d7b9e58a00",
   "metadata": {},
   "source": [
    "Automatic Chain of Thought or Auto-CoT automatically generates the intermediate reasoning steps by utilizing a database of diverse questions grouped into clusters.\n",
    "\n",
    "Auto-CoT goes through two main stages:\n",
    "\n",
    "1. Question Clustering: First, they partition questions of a given dataset into a few clusters. So, if people asked the computer program a bunch of questions about baking, like \"How do I mix the ingredients?\" and \"What temperature should I bake a pie at?\" these would go into different groups.\n",
    "   \n",
    "2. Demonstration Sampling: Once they have these question groups, they pick one question from each group and use Zero-Shot CoT prompt (basically the “Let’s think step by step” prompt). This way, the computer program generates clear and straightforward instructions on auto-pilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342d352-f272-4d6e-9180-e2885e3372a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('data/auto-cot.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c998a0-9e99-40a8-ab63-7de3af2abefe",
   "metadata": {},
   "source": [
    "# What's considered complex reasoning for LLMs today?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8dfef-da46-4eaf-a585-47d490da857e",
   "metadata": {},
   "source": [
    "\n",
    "- If we ask GPT-4 today to solve for x in the equation (64 = 2 + 5x + 32), it will solve it without any examples given.\n",
    "\n",
    "- This may look like a simple math problem, but at the beginning of 2023 this was a very challenging problem even for GPT-4.\n",
    "\n",
    "- These days, it seems like the model automatically provides step-by-step answers to most reasoning questions by default. Go ahead, try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffa423-abfc-412e-a606-4fe9f061e27a",
   "metadata": {},
   "source": [
    "# Limitations of CoT prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa02910-ccde-43bd-8c44-be86b28b288a",
   "metadata": {},
   "source": [
    "- There is no guarantee of correct reasoning paths\n",
    "- No matter the prompt engineering technique you pick for your project, it's important to experiment, test, and verify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464ecd7",
   "metadata": {},
   "source": [
    "## Example Use Case of COT Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0bf153",
   "metadata": {},
   "source": [
    "\n",
    "The below `greet_me` function is a simple Python function designed to return a greeting message. Let's break down the function and the example usage step by step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet_me(name,excitement):\n",
    "            return \"Hello, \" + (name*excitement) + \"!\"\n",
    "print(greet_me(\"maruti\"+\"!!\",3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d074f1",
   "metadata": {},
   "source": [
    "Let's ask gpt-3 to return a output from this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot example for extracting names and occupations\n",
    "normal_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpul assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "        ### Print the output from the below code\n",
    "        ```\n",
    "        def greet_me(name,excitement):\n",
    "            return \"Hello, \" + (name*excitement) + \"!\"\n",
    "        \n",
    "        print(greet_me(\"maruti\"+\"!!\",3))\n",
    "        ```\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the function with zero-shot prompting\n",
    "response = chat_completion_request(messages=normal_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca3d8c",
   "metadata": {},
   "source": [
    "As visible from above it did a bad job on giving the response/output from the function.<br>\n",
    "In these scenarios let's ask it to break down its thought process and then see what response it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f58830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot example for extracting names and occupations\n",
    "normal_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpul assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "        ### Print the output from the below code\n",
    "        ```\n",
    "        def greet_me(name,excitement):\n",
    "            return \"Hello, \" + (name*excitement) + \"!\"\n",
    "        \n",
    "        print(greet_me(\"maruti\"+\"!!\",3))\n",
    "        ```\n",
    "        Carefully think about it step by step and print the output from the function\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the function with zero-shot prompting\n",
    "response = chat_completion_request(messages=normal_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2fd78",
   "metadata": {},
   "source": [
    "It was able to get to the exact response when there is a reasoning chain it followed. Similar to these use cases , if you explain a LLM a thought process which it has to follow for a domain-specific question you would notice that it is able to generalize on solving specific kind of questions.<br>\n",
    "\n",
    "A Reference reading for this -> [here](https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b17ec",
   "metadata": {},
   "source": [
    "## React"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa40926551cff12",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Reference reading [here](https://www.promptingguide.ai/techniques/react) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa20b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "tools = load_tools([\"google-serper\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"get the median of the running lengths of the latest 5 blockbuster films in hollywood\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
