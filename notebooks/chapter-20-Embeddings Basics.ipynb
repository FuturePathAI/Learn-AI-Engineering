{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32afd345-065b-4ad7-8a10-bc593b483163",
   "metadata": {},
   "source": [
    "# What are Embeddings?\n",
    "\n",
    "- Embeddings act like a unique language translator for machines, taking diverse forms of data (like words, documents, images, people, ads, and more) and transforming them into a compact series of numbers in a smaller dimensional space.\n",
    "- This transformation manages to hold on to the crucial 'meaning' or 'context' of these data forms, which is what we call capturing their semantic information.\n",
    "- The real strength of embeddings comes from their ability to place similar data points close to each other and dissimilar ones farther apart in this compact space. It's like clustering related items in a neighborhood and separating distinct ones into different districts. \n",
    "\n",
    "- Sure, let's explore this in a hypothetical 6-dimensional embedding space. The embeddings (vectors) for the words \"king\", \"queen\", and \"apple\" could look something like this:\n",
    "\n",
    "1. king: [1.5, 2.2, 0.9, 1.1, 0.7, 1.3]\n",
    "2. queen: [1.6, 2.3, 0.85, 1.2, 0.75, 1.4]\n",
    "3. apple: [3.2, 4.1, 3.9, 3.5, 4.0, 3.8]\n",
    "\n",
    "Each number list (vector) here represents a point in our 6-dimensional space. The numbers for \"king\" and \"queen\" are closer to each other in this six-dimensional space compared to \"apple\". This indicates that \"king\" and \"queen\" are semantically more similar to each other than either is to \"apple\", which aligns with our intuitive understanding of the meanings of these words.\n",
    "\n",
    "This is the essence of how word embeddings capture semantic information, though, in actual practice, the vectors are in much higher-dimensional spaces (often 100s or 1000s of dimensions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e1e15-f267-4526-8bd2-70f7c2f9ff56",
   "metadata": {},
   "source": [
    "# How Are Embeddings Made?\n",
    "\n",
    "- Creating embeddings involves transforming discrete tokens (like words, sentences, or entire documents) into continuous vectors in a high-dimensional space. \n",
    "- These vectors capture the semantic similarities between the tokens. Let's dive into the process of creating word embeddings using a popular method: `Word2Vec`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c45a998-ea65-45ab-bb4d-e51fc2a59f76",
   "metadata": {},
   "source": [
    "### Step1: Text Pre-Processing\n",
    "\n",
    "- Before training, raw text data must be cleaned and prepared. \n",
    "- This involves removing punctuation, lowercasing, tokenizing (converting sentences into words), removing stop words, and possibly lemmatizing (reducing words to their base or root form).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ffa50-5a66-44c6-9027-599161571865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stop words and lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# Sample text\n",
    "sample_text = \"Natural language processing enables computers to understand human language.\"\n",
    "tokens = preprocess_text(sample_text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42509d7c-e4d8-40c6-8eee-95b5366432e0",
   "metadata": {},
   "source": [
    "### Step 2: Preparing Data for Word2Vec\n",
    "\n",
    "- Word2Vec requires a sequence of tokenized sentences for training. \n",
    "- If you're working with a large corpus, each document should be tokenized into sentences, and then each sentence tokenized into words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3d618-7ab7-4fff-a471-25afab73f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'raw_text' is a large string containing all your text\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Tokenize the raw text into sentences\n",
    "sentences = sent_tokenize(raw_text)\n",
    "\n",
    "# Preprocess each sentence and collect them\n",
    "preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45014e0a-7a87-4315-b7f1-55273048c1a9",
   "metadata": {},
   "source": [
    "### Step 3: Understanding Word2Vec Parameters\n",
    "\n",
    "When initializing Word2Vec, key parameters to consider are:\n",
    "\n",
    "- `vector_size`: Dimensionality of the word vectors.\n",
    "- `window`: Maximum distance between the current and predicted word within a sentence.\n",
    "- `min_count`: Ignores all words with total frequency lower than this.\n",
    "- `workers`: Number of worker threads to train the model.\n",
    "- `sg`: Training algorithm: 1 for Skip-Gram, 0 for CBOW.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee37ec-23e2-42ab-b1d6-93ee70d587af",
   "metadata": {},
   "source": [
    "### Step 4: Training the Model\n",
    "\n",
    "- Using Gensim's Word2Vec implementation, train the model on the preprocessed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139344f-f2c5-47ee-acb2-9eb5562d0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(preprocessed_sentences, vector_size=100, window=5, min_count=2, workers=4, sg=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed221e5-dded-4928-8625-6d33fcacc375",
   "metadata": {},
   "source": [
    "### Step 5: Exploring and Visualizing Embeddings\n",
    "\n",
    "- After training, explore the embeddings by checking similar words and visualizing the word vectors to get a sense of how words are positioned relative to each other in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab26278-3962-4108-9e8d-b4855d51cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words\n",
    "print(model.wv.most_similar('computer'))\n",
    "\n",
    "# Visualize embeddings using t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = list(model.wv.key_to_index.keys())\n",
    "word_vectors = [model.wv[word] for word in words]\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "word_vecs_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(word_vecs_2d[i, 0], word_vecs_2d[i, 1])\n",
    "    plt.annotate(word, xy=(word_vecs_2d[i, 0], word_vecs_2d[i, 1]), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5ea50-a08f-4cbe-a0d5-42633d1812c4",
   "metadata": {},
   "source": [
    "- This visualization helps illustrate the principle that semantically similar words cluster together in the embedding space. \n",
    "- For example, \"king\" and \"queen\" might be closer together, whereas \"apple\" might be far from them but closer to \"orange\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c804b-a84c-43be-a9ab-c5d8c206c4ad",
   "metadata": {},
   "source": [
    "# Word-level Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb201f-3dff-41de-98b8-68c360c15170",
   "metadata": {},
   "source": [
    "Word-level embeddings represent individual words in a high-dimensional space. These embeddings capture the semantic properties of words based on their usage in the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff86aba-2db6-4967-9849-67ab04f1270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode a word\n",
    "word = 'king'\n",
    "inputs = tokenizer(word, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get the word embedding\n",
    "word_embedding = outputs.last_hidden_state\n",
    "print(\"Word Embedding for 'king':\", word_embedding[:, 1:-1, :].squeeze().detach().numpy()) # Squeeze to remove batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8a7c6-2f14-4de2-859e-1f3ea745e774",
   "metadata": {},
   "source": [
    "# Sentence-level Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21533419-9f64-4191-b049-9b8aec6aecd8",
   "metadata": {},
   "source": [
    "Sentence-level embeddings represent the entire sentences, capturing not just the semantics of individual words but also how those words are used together in a sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36740416-52f8-4448-a242-f01866ad45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define sentences\n",
    "sentences = [\n",
    "    \"The king rules the kingdom.\",\n",
    "    \"A piece of cake.\",\n",
    "    \"I have a dream.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Embedding: {embedding[:5]}...\")  # Display first 5 elements for brevity\n",
    "    print(\"Embedding length:\", len(embedding), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7771d-904e-43e3-9aaf-d2c9025ad536",
   "metadata": {},
   "source": [
    "- In the word-level embedding example, we focused on extracting an embedding for a single word, 'king'. This embedding captures the semantic essence of 'king' based on its context in the training data.\n",
    "\n",
    "\n",
    "- For the sentence-level embeddings, we processed an entire sentence, 'The king rules the kingdom.'. The resulting embedding is an aggregate (in this case, a mean) of all token embeddings in the sentence, capturing the overall semantic content of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f8b54-0648-4170-aef0-81fb42991032",
   "metadata": {},
   "source": [
    "# Types of Embeddings\n",
    "\n",
    "### Word2Vec\n",
    "- Word2Vec, developed by Mikolov et al. at Google, is a predictive model for generating word embeddings. \n",
    "- Word2Vec uses a shallow neural network to learn word associations from a large corpus of text. It operates on the principle that \"a word is known by the company it keeps,\" focusing on learning embeddings that predict nearby words.\n",
    "- It uses either a Continuous Bag of Words (CBOW) or Skip-Gram model:\n",
    "    - **Skip-Gram Model**: Predicts surrounding context words given a target word.\n",
    "    - **CBOW Model**: Predicts the target word from a set of context words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ad1d6-7246-42dd-924c-80f1c5064e01",
   "metadata": {},
   "source": [
    "Imagine a sentence \"Cats love to play.\" Word2Vec can learn to predict \"love\" when given \"Cats\" and \"play\" as context in the Skip-Gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22fd41-c56f-47e5-8ba6-46d0c90b10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"bark\"]]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access the word vector for 'cat'\n",
    "vector_cat = model.wv['cat']\n",
    "print(vector_cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf385521-9faf-40ec-acf4-60968ed0d015",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "- GloVe (Global Vectors for Word Representation), developed by Stanford, is an unsupervised learning model for obtaining vector representations for words by aggregating global word-word co-occurrence statistics from a corpus.\n",
    "\n",
    "    - Co-occurrence Matrix: Constructs a matrix that captures how often pairs of words occur together in the context of the entire corpus.\n",
    "    - Matrix Factorization: Reduces this matrix to produce a lower-dimensional representation (embeddings) that captures major semantic relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00c2a2-e4c4-4d13-89aa-62e93b742649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Convert GloVe file to Word2Vec format\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "# Load the converted model\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "# Access the word vector for 'cat'\n",
    "vector_cat = model['cat']\n",
    "print(vector_cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3524b9c3-90dd-4e2c-8c4b-08241b3a1bc9",
   "metadata": {},
   "source": [
    "### FastText\n",
    "- FastText, developed by Facebook Research, extends Word2Vec to consider subword information.\n",
    "- FastText breaks words into a bag of character n-grams. This approach allows FastText to generate word embeddings for words not seen during training, making it particularly useful for handling rare words or morphologically rich languages.\n",
    "\n",
    "    - Subword N-grams: Breaks words into smaller chunks and learns embeddings for these sub-parts, which are then aggregated to form the word's embedding.\n",
    "    - Handling of Rare Words: By considering subwords, FastText can generate embeddings for rare or out-of-vocabulary words based on their compositional parts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31975b-cb99-4549-a694-fa5aaf04001b",
   "metadata": {},
   "source": [
    "\n",
    "For the word \"unbelievable,\" even if it wasn't in the training corpus, FastText could derive its meaning from the subwords \"un,\" \"believ,\" \"able,\" etc., creating an embedding that reflects its semantic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14963f27-ec85-4304-a91a-5c959f2f57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"bark\"]]\n",
    "\n",
    "# Train a FastText model\n",
    "model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access the word vector for 'cat'\n",
    "vector_cat = model.wv['cat']\n",
    "print(vector_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291a56d-a330-40fd-aaeb-ff7a1833cb5b",
   "metadata": {},
   "source": [
    "# Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf76e29-5dd3-4f7e-b603-0e76ee036f82",
   "metadata": {},
   "source": [
    "Each embedding technique has its unique strengths, making them suitable for different NLP tasks. However, they share common limitations, such as the inability to capture word sense disambiguation (words with multiple meanings) effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a44a8-7fd2-44ca-a9ed-720f546366e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Dogs love to play in the park.\",\n",
    "    \"I love my cat and my dog.\",\n",
    "    \"The bank of the river was flooded.\",\n",
    "    \"He went to the bank to deposit money.\"\n",
    "]\n",
    "\n",
    "# Tokenization of sentences\n",
    "tokens = [sentence.lower().split() for sentence in sentences]\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Exploring word embeddings\n",
    "word_vectors = model.wv\n",
    "\n",
    "# Example: Find most similar words to 'bank'\n",
    "similar_words = word_vectors.most_similar('bank', topn=5)\n",
    "print(\"Words similar to 'bank':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46ed7b-cc6d-47d7-a019-beca50242a59",
   "metadata": {},
   "source": [
    "While Word2Vec is powerful for capturing semantic relationships, it assigns a single vector per word, regardless of its different meanings in various contexts. For example, 'bank' in our sentences refers to both the land alongside a river and a financial institution, but Word2Vec cannot distinguish these senses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944bdc5d-6171-49b5-9e11-e2915efa630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'bank' has been included in the model vocabulary\n",
    "print(\"Context vectors for 'bank':\", word_vectors['bank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342815e-8cb4-44e7-9c95-e5c7a232fe1d",
   "metadata": {},
   "source": [
    "This output will show a single vector representation for 'bank,' which is an average of its contexts in the training data. Because Word2Vec does not generate different embeddings for the different meanings of 'bank,' it can't disambiguate between them based on context alone.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bdaf2-e11f-46d7-b48a-d65d1ccdd729",
   "metadata": {},
   "source": [
    "# Overcoming Limitations\n",
    "More advanced models like BERT or ELMo offer solutions to this limitation by providing context-dependent embeddings, where the representation of 'bank' would differ based on its usage in a sentence. These models generate different embeddings for the same word in different contexts, better capturing its various meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eed077-0745-46ef-a92e-8bf0d6d97c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
