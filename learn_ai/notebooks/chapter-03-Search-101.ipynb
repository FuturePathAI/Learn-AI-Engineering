{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search: Retrieval & Ranking\n",
    "\n",
    "Based on [Q&A using Embeddings](https://cookbook.openai.com/examples/question_answering_using_embeddings) from OpenAI Cookbook.\n",
    "\n",
    "### Why do we even search? \n",
    "\n",
    "For every time, we need the model to use some information which it has not already seen during training. This is common when there is something new in the world, or when the model is being used in a new context e.g. your company's internal data.\n",
    "\n",
    "> GPT can learn knowledge in two ways:\n",
    ">\n",
    "> - Via model weights (i.e., fine-tune the model on a training set)\n",
    "> - Via model inputs (i.e., insert the knowledge into an input message)\n",
    "> \n",
    "> Although fine-tuning can feel like the more natural option—training on data is how GPT learned all of its other knowledge, after all—we generally do not recommend it as a way to teach the model knowledge. Fine-tuning is better suited to teaching specialized tasks or styles, and is less reliable for factual recall.\n",
    "> \n",
    "> As an analogy, model weights are like long-term memory. When you fine-tune a model, it's like studying for an exam a week away. When the exam arrives, the model may forget details, or misremember facts it never read.\n",
    "> \n",
    "> In contrast, message inputs are like short-term memory. When you insert knowledge into a message, it's like taking an exam with open notes. With notes in hand, the model is more likely to arrive at correct answers.\n",
    "\n",
    "### Retrieval Augmented Generation\n",
    "\n",
    "While we will cover this in more detail in the later chapters, it's worth mentioning here that the system can be combined with retrieved information from a database and then generate a response based on that information. This is called retrieval augmented generation.\n",
    "\n",
    "![](../assets/Retrieval%20Augemented%20Generation.gif)\n",
    "\n",
    "Following are the steps to perform retrieval augmented generation:\n",
    "\n",
    "## Retrieval\n",
    "1. Prepare search data: Prepare a dataset of documents that you want to search through.\n",
    "2. Create embeddings: Create embeddings for each document in the dataset.\n",
    "3. Prepare index: Create an index of the embeddings, this will allow you to search through the documents quickly.\n",
    "4. Search: Search through the documents using the embeddings.\n",
    "\n",
    "## Generation\n",
    "5. Generate: Use the retrieved documents to generate a response.\n",
    "\n",
    "Here, we'll quickly introduce a simplified view of the retrieval using the OpenAI API next:\n",
    "\n",
    "1. Prepare search data: You need to prepare a dataset of documents that you want to search through. This could be a list of documents, a list of paragraphs, or a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Path(\"../data/paul_graham/paul_graham_essay.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the summer of 2016, Paul Graham likely continued his work as a venture capitalist and co-founder of Y Combinator, a startup accelerator. He may have also been involved in mentoring and advising startups, as well as writing essays and giving talks on entrepreneurship and technology.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask(query: str, model: str = MODEL) -> str:\n",
    "    \"\"\"Return the response to a query.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "ask(\"What did Paul Graham do in Summer of 2016?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Processing: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16534"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_tokens(text: str, model: str = MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "num_tokens(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model can take a maximum of 16,385 which is less than the number of tokens in the document. We need to chunk the document into smaller pieces.\n",
    "\n",
    "Here, we'll simply split the document into approximate chunks of 1024 tokens each. This heuristic is based on empirical experiments [here](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5) by the good folks at LlamaIndex.\n",
    "\n",
    "We'd also recommend using [ChunkWiz](https://chunkviz.up.railway.app/) to build your intuition about the chunking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,\n",
       " 'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=96,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([text])\n",
    "context_text = [t.page_content for t in texts]\n",
    "len(context_text), context_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 101\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "def create_embeddings(texts: List[str], model: str = EMBEDDING_MODEL) -> List[str]:\n",
    "    \"\"\"Return the embeddings for a list of texts.\"\"\"\n",
    "    return client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "response = create_embeddings(context_text)\n",
    "print(f\"Number of documents: {len(response.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching with Voyager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 1536)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare a list of embeddigns from the response object\n",
    "\n",
    "vectors = [d.embedding for d in response.data]\n",
    "len(vectors), len(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<voyager.FloatIndex space=Cosine num_dimensions=1536 storage_data_type=Float32>\n"
     ]
    }
   ],
   "source": [
    "from voyager import Index, Space\n",
    "\n",
    "# Create an empty Index object that can store vectors:\n",
    "index = Index(Space.Cosine, num_dimensions=1536)\n",
    "index.add_items(vectors)\n",
    "\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = create_embeddings([\"What did Paul Graham do in Summer of 2016?\"]).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72 71]\n",
      "[0.5492338 0.5515524]\n"
     ]
    }
   ],
   "source": [
    "# # Find the two closest elements:\n",
    "neighbors, distances = index.query(query_vector, k=2)\n",
    "print(neighbors)  # => [0, 1]\n",
    "print(distances)  # => [0.0, 125.0]\n",
    "\n",
    "# # Save the index to disk to reload later (or in Java!)\n",
    "# index.save(\"output_file.voy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I could not find an answer.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_with_context(query: str, context: List[str], model: str = MODEL) -> str:\n",
    "    introduction = \"\"\"Use the below writing from Paul Graham to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"\"\"\n",
    "    question = f\"\\n\\nQuestion: {query}\\n\\n\"\n",
    "    context = \"\\n\\n\".join(context)\n",
    "    return ask(introduction + context + question, model)\n",
    "\n",
    "selected_context = [context_text[neighbors[0]]]\n",
    "ask_with_context(\"What did Paul Graham do in Summer of 2016?\", selected_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the model response was more helpful than the search results. But in many cases, the search results will be more helpful than the model response. We encourage you to \"improve retrieval\" and try different search strategies to see how the model responds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
