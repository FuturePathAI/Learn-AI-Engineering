{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e67f200",
   "metadata": {},
   "source": [
    "# Function Calling -- Format JSON\n",
    "\n",
    "Function calling is my favourite, open secret about modern LLMs: They can be used to communicate with other systems. \n",
    "\n",
    "How do we do that? By parsing your information into a JSON, which can then be used to interact with your APIs. There are more straight forward use cases for JSON output as well e.g. parsing a text into structured data.\n",
    "\n",
    "The remainder of this notebook is directly from the [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6748ab2",
   "metadata": {},
   "source": [
    "\n",
    "This notebook covers how to use the Chat Completions API in combination with external functions to extend the capabilities of GPT models.\n",
    "\n",
    "`tools` is an optional parameter in the Chat Completion API which can be used to provide function specifications. The purpose of this is to enable models to generate function arguments which adhere to the provided specifications. Note that the API will not actually execute any function calls. It is up to developers to execute function calls using model outputs.\n",
    "\n",
    "Within the `tools` parameter, if the `functions` parameter is provided then by default the model will decide when it is appropriate to use one of the functions. The API can be forced to use a specific function by setting the `tool_choice` parameter to `{\"name\": \"<insert-function-name>\"}`. The API can also be forced to not use any function by setting the `tool_choice` parameter to `\"none\"`. If a function is used, the output will contain `\"finish_reason\": \"function_call\"` in the response, as well as a `tool_choice` object that has the name of the function and the generated function arguments.\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook contains the following 2 sections:\n",
    "\n",
    "- **How to generate function arguments:** Specify a set of functions and use the API to generate function arguments.\n",
    "- **How to call functions with model generated arguments:** Close the loop by actually executing functions with model generated arguments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64c85e26",
   "metadata": {},
   "source": [
    "## How to generate function arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from termcolor import colored\n",
    "\n",
    "load_dotenv()\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "client = OpenAI()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69ee6a93",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "First let's define a few utilities for making calls to the Chat Completions API and for maintaining and keeping track of the conversation state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "745ceec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d1c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_conversation(messages):\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "        \"function\": \"magenta\",\n",
    "    }\n",
    "    \n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"function\":\n",
    "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29d4e02b",
   "metadata": {},
   "source": [
    "### Basic concepts\n",
    "\n",
    "Let's create some function specifications to interface with a hypothetical weather API. We'll pass these function specification to the Chat Completions API in order to generate function arguments that adhere to the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e25069",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"format\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\", \"format\"],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_n_day_weather_forecast\",\n",
    "            \"description\": \"Get an N-day weather forecast\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"format\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                    },\n",
    "                    \"num_days\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The number of days to forecast\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\", \"format\", \"num_days\"]\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfc39899",
   "metadata": {},
   "source": [
    "If we prompt the model about the current weather, it will respond with some clarifying questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "518d6827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='Sure, could you please provide me with the location?', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, tools=tools\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message\n",
    "messages.append(assistant_message)\n",
    "assistant_message\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c999375",
   "metadata": {},
   "source": [
    "Once we provide the missing information, it will generate the appropriate function arguments for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c42a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_oNegav2r7A6xPALkwkolEMLE', function=Function(arguments='{\\n  \"location\": \"Glasgow, Scotland\",\\n  \"format\": \"celsius\"\\n}', name='get_current_weather'), type='function')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"I'm in Glasgow, Scotland.\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, tools=tools\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message\n",
    "messages.append(assistant_message)\n",
    "assistant_message\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c14d4762",
   "metadata": {},
   "source": [
    "By prompting it differently, we can get it to target the other function we've told it about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa232e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='Sure, I can help you with that. Please provide the number of days you want to forecast.', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in Glasgow, Scotland over the next x days\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, tools=tools\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message\n",
    "messages.append(assistant_message)\n",
    "assistant_message\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6172ddac",
   "metadata": {},
   "source": [
    "Once again, the model is asking us for clarification because it doesn't have enough information yet. In this case it already knows the location for the forecast, but it needs to know how many days are required in the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d8a543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_OV9VdLnHnM8PEkivGHtQVmrT', function=Function(arguments='{\\n  \"location\": \"Glasgow, Scotland\",\\n  \"format\": \"celsius\",\\n  \"num_days\": 5\\n}', name='get_n_day_weather_forecast'), type='function')]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"5 days\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, tools=tools\n",
    ")\n",
    "chat_response.choices[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b758a0a",
   "metadata": {},
   "source": [
    "#### Forcing the use of specific functions or no function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "412f79ba",
   "metadata": {},
   "source": [
    "We can force the model to use a specific function, for example get_n_day_weather_forecast by using the function_call argument. By doing so, we force the model to make assumptions about how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "559371b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_WaBjmbCPPcR9Ni8y1ZNyGhUR', function=Function(arguments='{\\n  \"location\": \"Toronto, Canada\",\\n  \"format\": \"celsius\",\\n  \"num_days\": 1\\n}', name='get_n_day_weather_forecast'), type='function')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this cell we force the model to use get_n_day_weather_forecast\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_n_day_weather_forecast\"}}\n",
    ")\n",
    "chat_response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7ab0f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_sfyfwJERT0XkfZ8GGAKpF18v', function=Function(arguments='{\\n  \"location\": \"Toronto, Canada\",\\n  \"format\": \"celsius\"\\n}', name='get_current_weather'), type='function')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we don't force the model to use get_n_day_weather_forecast it may not\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, tools=tools\n",
    ")\n",
    "chat_response.choices[0].message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bd70e48",
   "metadata": {},
   "source": [
    "We can also force the model to not use a function at all. By doing so we prevent it from producing a proper function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfe54e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='{ \"location\": \"Toronto, Canada\", \"format\": \"celsius\" }', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give me the current weather (use Celcius) for Toronto, Canada.\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, tools=tools, tool_choice=\"none\"\n",
    ")\n",
    "chat_response.choices[0].message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b616353b",
   "metadata": {},
   "source": [
    "### Parallel Function Calling\n",
    "\n",
    "Newer models like gpt-4-1106-preview or gpt-3.5-turbo-1106 can call multiple functions in one turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "380eeb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_5aJObKpRPbGWotcboJbuayjS', function=Function(arguments='{\"location\": \"San Francisco\", \"format\": \"celsius\", \"num_days\": 4}', name='get_n_day_weather_forecast'), type='function'),\n",
       " ChatCompletionMessageToolCall(id='call_Sp92VXTDYrkpau7p5UwCbiRc', function=Function(arguments='{\"location\": \"Glasgow\", \"format\": \"celsius\", \"num_days\": 4}', name='get_n_day_weather_forecast'), type='function')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in San Francisco and Glasgow over the next 4 days\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, tools=tools, model='gpt-3.5-turbo-1106'\n",
    ")\n",
    "\n",
    "assistant_message = chat_response.choices[0].message.tool_calls\n",
    "assistant_message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d89073c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Checkout other [notebook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb) that demonstrates how to use the Chat Completions API and functions for knowledge retrieval to interact conversationally with a knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfebf7e",
   "metadata": {},
   "source": [
    "# Using Function Calling with Pydantic\n",
    "\n",
    "Pydantic is a data validation and settings management using Python type annotations. It is a library that allows you to validate data and settings using Python type annotations. It is a great library to use with FastAPI, but it can be used with any Python project. We use instructor (https://jxnl.github.io/instructor) to demonstrate how to use Pydantic with Function Calling.\n",
    "\n",
    "As an illustration, we will use a complex task: Graph Construction -- but a simple example, so that you can self verify the errors with the model outputs. \n",
    "\n",
    "## Graph Construction\n",
    "\n",
    "In this guide, we demonstrate how to extract and resolve entities from a sample legal contract. Then, we visualize these entities and their dependencies as an entity graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40c3998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Property(BaseModel):\n",
    "    key: str\n",
    "    value: str\n",
    "    resolved_absolute_value: str\n",
    "\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    id: int = Field(\n",
    "        ...,\n",
    "        description=\"Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities\",\n",
    "    )\n",
    "    subquote_string: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution\",\n",
    "    )\n",
    "    entity_title: str\n",
    "    properties: List[Property] = Field(\n",
    "        ..., description=\"List of properties of the entity\"\n",
    "    )\n",
    "    dependencies: List[int] = Field(\n",
    "        ...,\n",
    "        description=\"List of entity ids that this entity depends  or relies on to resolve it\",\n",
    "    )\n",
    "\n",
    "class DocumentExtraction(BaseModel):\n",
    "    entities: List[Entity] = Field(\n",
    "        ...,\n",
    "        description=\"Body of the answer, each fact should be a separate object with a body and a list of sources\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153efd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d3cc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the patch to the OpenAI client\n",
    "# enables response_model keyword\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "\n",
    "def ask_ai(content) -> DocumentExtraction:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        response_model=DocumentExtraction,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Extract and resolve a list of entities from the following document:\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            },\n",
    "        ],\n",
    "    )  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50de41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "def generate_html_label(entity: Entity) -> str:\n",
    "    rows = [\n",
    "        f\"<tr><td>{prop.key}</td><td>{prop.resolved_absolute_value}</td></tr>\"\n",
    "        for prop in entity.properties\n",
    "    ]\n",
    "    table_rows = \"\".join(rows)\n",
    "    return f\"<<table border='0' cellborder='1' cellspacing='0'><tr><td colspan='2'><b>{entity.entity_title}</b></td></tr>{table_rows}</table>>\"\n",
    "\n",
    "\n",
    "def generate_graph(data: DocumentExtraction):\n",
    "    dot = Digraph(comment=\"Entity Graph\", node_attr={\"shape\": \"plaintext\"})\n",
    "\n",
    "    for entity in data.entities:\n",
    "        label = generate_html_label(entity)\n",
    "        dot.node(str(entity.id), label)\n",
    "\n",
    "    for entity in data.entities:\n",
    "        for dep_id in entity.dependencies:\n",
    "            dot.edge(str(entity.id), str(dep_id))\n",
    "\n",
    "    dot.render(\"entity.gv\", view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9794f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "Sample Legal Contract\n",
    "Agreement Contract\n",
    "\n",
    "This Agreement is made and entered into on 2020-01-01 by and between Company A (\"the Client\") and Company B (\"the Service Provider\").\n",
    "\n",
    "Article 1: Scope of Work\n",
    "\n",
    "The Service Provider will deliver the software product to the Client 30 days after the agreement date.\n",
    "\n",
    "Article 2: Payment Terms\n",
    "\n",
    "The total payment for the service is $50,000.\n",
    "An initial payment of $10,000 will be made within 7 days of the the signed date.\n",
    "The final payment will be due 45 days after [SignDate].\n",
    "\n",
    "Article 3: Confidentiality\n",
    "\n",
    "The parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date.\n",
    "\n",
    "Article 4: Termination\n",
    "\n",
    "The contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the [DeliveryDate].\n",
    "\"\"\"  # Your legal contract here\n",
    "model = ask_ai(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89efa10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d2af4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_content = \"\"\"\n",
    "# Technical Document: User Profile Service with Horizontal Scaling\n",
    "\n",
    "## Overview\n",
    "\n",
    "The User Profile Service is a microservice designed to manage user profiles in a large-scale web application. It provides functionalities such as creating, updating, retrieving, and deleting user profile information. Given the dynamic nature of user interactions and the potential for high traffic, this service is designed with horizontal scaling capabilities to ensure high availability, resilience, and consistent performance under varying loads.\n",
    "\n",
    "## Service Description\n",
    "\n",
    "### Functional Requirements\n",
    "\n",
    "1. **Create Profile**: Allows creating a new user profile with details like name, email, and preferences.\n",
    "2. **Update Profile**: Permits modifications to existing profiles.\n",
    "3. **Retrieve Profile**: Enables fetching profile details for a given user ID.\n",
    "4. **Delete Profile**: Supports removal of a user profile from the system.\n",
    "\n",
    "### Non-Functional Requirements\n",
    "\n",
    "1. **Scalability**: The service must scale horizontally to handle spikes in traffic.\n",
    "2. **Performance**: Response times should remain consistent under load.\n",
    "3. **Availability**: Designed for 99.99% uptime.\n",
    "4. **Security**: Ensure data protection and secure access.\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "### Components\n",
    "\n",
    "1. **API Gateway**: Serves as the entry point for all client requests, routing them to the appropriate service instance.\n",
    "2. **Service Instances**: Multiple instances of the User Profile Service, each capable of handling requests independently.\n",
    "3. **Load Balancer**: Distributes incoming requests evenly across service instances to ensure load distribution and high availability.\n",
    "4. **Database Cluster**: A horizontally scalable database that stores user profile information. It ensures data consistency and high availability through replication and sharding techniques.\n",
    "5. **Cache Layer**: An in-memory cache (e.g., Redis) to reduce database load and improve response times for frequently accessed data.\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "1. A client request hits the API Gateway.\n",
    "2. The request is forwarded to the Load Balancer.\n",
    "3. The Load Balancer selects a Service Instance based on load distribution algorithms.\n",
    "4. The Service Instance processes the request. It may interact with the Cache Layer or the Database Cluster as needed.\n",
    "5. The response is sent back to the client following the reverse path.\n",
    "\n",
    "## Horizontal Scaling Strategy\n",
    "\n",
    "### Service Instances\n",
    "\n",
    "- **Auto-Scaling**: Service instances are scaled horizontally based on CPU and memory usage metrics. When thresholds are exceeded, new instances are automatically spawned across multiple servers or cloud infrastructure.\n",
    "\n",
    "### Database Cluster\n",
    "\n",
    "- **Sharding**: The database is partitioned into shards, each holding a subset of the data. This distributes the load and enables the database to grow horizontally.\n",
    "- **Replication**: Each shard is replicated across multiple nodes to ensure data availability and fault tolerance.\n",
    "\n",
    "### Cache Layer\n",
    "\n",
    "- The cache layer is designed to scale out by adding more nodes, which allows for distributing cache data and handling more concurrent connections.\n",
    "\n",
    "## Technologies Used\n",
    "\n",
    "- **API Gateway**: NGINX, Amazon API Gateway\n",
    "- **Service Implementation**: Node.js, Express.js\n",
    "- **Load Balancer**: HAProxy, AWS Elastic Load Balancing\n",
    "- **Database**: MongoDB with sharding, Cassandra\n",
    "- **Cache**: Redis Cluster\n",
    "- **Containerization**: Docker\n",
    "- **Orchestration**: Kubernetes for managing containerized service instances\n",
    "- **Monitoring and Logging**: Prometheus, Grafana, ELK Stack\n",
    "\n",
    "## Deployment and Operations\n",
    "\n",
    "- The service is deployed on a Kubernetes cluster, leveraging its auto-scaling capabilities for service instances.\n",
    "- Continuous Integration and Continuous Deployment (CI/CD) pipelines are used for automated testing and deployment.\n",
    "- Comprehensive monitoring and logging mechanisms are in place for real-time performance tracking and anomaly detection.\n",
    "\n",
    "## Security Considerations\n",
    "\n",
    "- All data in transit and at rest are encrypted using industry-standard encryption algorithms.\n",
    "- Access to the service is secured using OAuth 2.0 for authentication and authorization.\n",
    "- Regular security audits and vulnerability assessments are conducted.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The User Profile Service is designed to be a robust, scalable, and secure microservice that can handle high loads and ensure consistent performance. By leveraging horizontal scaling strategies, modern technologies, and best practices in system design, this service can adapt to the growing demands of a large-scale web application.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeeeff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_model = ask_ai(example_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0761cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph(technical_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fa8927",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "The entity model for the technical diagram can be made richer, with a focus on tools, function calls/data flow, storage and other components. This can be used to generate a technical diagram from a textual description. We encourage you to attempt this task as a challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
