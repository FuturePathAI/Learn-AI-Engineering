{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdcc1fec-51e2-4d7d-8fdb-7447fee9e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learn_ai.scripts.utils import chat_completion_request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6d42f-4b18-496c-af16-c1a4fe158222",
   "metadata": {},
   "source": [
    "# Lesson 1: Zero Shot Prompting\n",
    "\n",
    "- The word \"shot\" is synonymous with \"example\"\n",
    "- Zero-shot prompting can be likened to requesting someone to undertake a task they have no prior experience in, without providing any examples or guidelines.\n",
    "- Consider the scenario where you ask a friend, who has never engaged in cooking, to bake a cake. You don't provide a recipe or any demonstrations; they must depend entirely on their existing knowledge or assumptions about baking.\n",
    "- In the realm of artificial intelligence, zero-shot prompting operates in a comparable manner. Here, the AI employs its inherent knowledge, acquired during its training phase, to attempt a task for which it hasn't been explicitly prepared or shown specific examples.\n",
    "- This approach tests the AI's ability to apply its general understanding to new and unseen challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3332064-e89b-4cb5-bc3d-9ca71e95c3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily Johnson [Educator]\n",
      "Michael Smith [Tech visionary]\n",
      "John Doe [Pastry chef]\n",
      "Jane Smith [Architect]\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot example for extracting names and occupations\n",
    "zero_shot_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a model that extracts names and occupations from text.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Read the following article and list the names and occupations in the format 'First Last [OCCUPATION]': \\\n",
    "                    'In today's town news, we celebrate Emily Johnson, who has dedicated over two decades to enlightening young \\\n",
    "                    minds in our local schools, being named Educator of the Year. Meanwhile, a visionary in the tech world, Michael Smith, \\\n",
    "                    is spearheading an innovative venture in sustainable technology. Also in the news, renowned pastry chef John Doe is set \\\n",
    "                    to host a baking masterclass, while Jane Smith, known for her architectural prowess, has unveiled plans for a new city park.'\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the function with zero-shot prompting\n",
    "zero_shot_response = chat_completion_request(messages=zero_shot_messages)\n",
    "print(zero_shot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615c2d1-6e4a-46f8-b439-8e7b31bf2cd4",
   "metadata": {},
   "source": [
    "# Lesson 2: Few Shot Prompting\n",
    "\n",
    "- Few-shot prompting is akin to asking someone to perform a task with the aid of a few examples.\n",
    "- The AI is presented with a small number of examples before tackling a new task. These examples serve as a learning aid, enabling the AI to grasp the task's context and desired output.\n",
    "- This method enhances the AI's ability to adapt its pre-existing knowledge to tasks it was not explicitly trained for, using the provided examples as a learning reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27c64560-930f-434f-a35b-545bfb3abcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 13:16:27,915 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that extracts names and occupations from text.'}, {'role': 'user', 'content': \"Example: Article text: 'John Doe, renowned for his exquisite pastries, has won numerous awards.' Output: John Doe [PASTRY CHEF]\"}, {'role': 'user', 'content': \"Example: Article text: 'Architect Jane Smith received acclaim for her innovative building designs.' Output: Jane Smith [ARCHITECT]\"}, {'role': 'user', 'content': \"Read the following article and list the names and occupations in the format 'First Last [OCCUPATION]': 'In today's town news, we celebrate Emily Johnson, who has dedicated over two decades to enlightening young minds in our local schools, being named Educator of the Year. Meanwhile, a visionary in the tech world, Michael Smith, is spearheading an innovative venture in sustainable technology. Also in the news, renowned pastry chef John Doe is set to host a baking masterclass, while Jane Smith, known for her architectural prowess, has unveiled plans for a new city park.'\"}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0, 'top_p': 1}}\n",
      "2024-01-19 13:16:27,923 [_trace.py:trace:45] DEBUG - close.started\n",
      "2024-01-19 13:16:27,926 [_trace.py:trace:45] DEBUG - close.complete\n",
      "2024-01-19 13:16:27,955 [_trace.py:trace:45] DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2024-01-19 13:16:28,014 [_trace.py:trace:45] DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x106e5c3d0>\n",
      "2024-01-19 13:16:28,015 [_trace.py:trace:45] DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1073446c0> server_hostname='api.openai.com' timeout=5.0\n",
      "2024-01-19 13:16:28,045 [_trace.py:trace:45] DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1073591b0>\n",
      "2024-01-19 13:16:28,046 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 13:16:28,047 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-19 13:16:28,048 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-19 13:16:28,048 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-19 13:16:28,049 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 13:16:28,956 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 19 Jan 2024 18:16:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'560'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159756'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'91ms'), (b'x-request-id', b'5c77a388b97a2fe4ebc7c62e32e655f4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'848111a7ac19b0bd-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-19 13:16:28,958 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-19 13:16:28,959 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-19 13:16:28,960 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-19 13:16:28,961 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-19 13:16:28,962 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-19 13:16:28,963 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily Johnson [EDUCATOR]\n",
      "Michael Smith [TECH VISIONARY]\n",
      "John Doe [PASTRY CHEF]\n",
      "Jane Smith [ARCHITECT]\n"
     ]
    }
   ],
   "source": [
    "# Few-shot example for extracting names and occupations\n",
    "few_shot_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a model that extracts names and occupations from text.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Example: Article text: 'John Doe, renowned for his exquisite pastries, has won numerous awards.' Output: John Doe [PASTRY CHEF]\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Example: Article text: 'Architect Jane Smith received acclaim for her innovative building designs.' Output: Jane Smith [ARCHITECT]\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Read the following article and list the names and occupations in the format 'First Last [OCCUPATION]': 'In today's town news, we celebrate Emily Johnson, who has dedicated over two decades to enlightening young minds in our local schools, being named Educator of the Year. Meanwhile, a visionary in the tech world, Michael Smith, is spearheading an innovative venture in sustainable technology. Also in the news, renowned pastry chef John Doe is set to host a baking masterclass, while Jane Smith, known for her architectural prowess, has unveiled plans for a new city park.'\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the function with few-shot prompting\n",
    "few_shot_response = chat_completion_request(messages=few_shot_messages)\n",
    "print(few_shot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816065f4-2258-4f85-9106-d7677f5a4794",
   "metadata": {},
   "source": [
    "## Limitations of Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b541551-812b-46ac-9ea4-87a8af54fa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 13:22:02,373 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that solves complex numerical reasoning problems.'}, {'role': 'user', 'content': 'The sum of odd numbers in this group minus the sum of even numbers results in an odd number: 4, 8, 9, 15, 12, 2, 1. The answer is False.'}, {'role': 'user', 'content': 'The sum of odd numbers in this group minus the sum of even numbers results in an odd number: 17, 10, 19, 4, 8, 12, 24. The answer is True.'}, {'role': 'user', 'content': 'The sum of odd numbers in this group minus twice the sum of even numbers results in an odd number: 15, 32, 5, 13, 82, 39, 67, 7, 1, 48, 26. The answer is?'}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-19 13:22:02,377 [_trace.py:trace:45] DEBUG - close.started\n",
      "2024-01-19 13:22:02,379 [_trace.py:trace:45] DEBUG - close.complete\n",
      "2024-01-19 13:22:02,381 [_trace.py:trace:45] DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2024-01-19 13:22:02,453 [_trace.py:trace:45] DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1073727d0>\n",
      "2024-01-19 13:22:02,455 [_trace.py:trace:45] DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1073446c0> server_hostname='api.openai.com' timeout=5.0\n",
      "2024-01-19 13:22:02,488 [_trace.py:trace:45] DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x107360490>\n",
      "2024-01-19 13:22:02,492 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 13:22:02,500 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-19 13:22:02,501 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-19 13:22:02,502 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-19 13:22:02,503 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 13:22:05,928 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 19 Jan 2024 18:22:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'3358'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159856'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'54ms'), (b'x-request-id', b'e9fb67e46e0a71f7dd7590250586c8e4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'848119d2095c44ea-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-19 13:22:05,931 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-19 13:22:05,932 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-19 13:22:05,933 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-19 13:22:05,934 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-19 13:22:05,935 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-19 13:22:05,936 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine if the statement is true or false, we need to calculate the sum of odd numbers and twice the sum of even numbers, and then check if their difference is an odd number.\n",
      "\n",
      "The odd numbers in the group are: 15, 5, 13, 39, 67, 7, 1.\n",
      "The sum of odd numbers = 15 + 5 + 13 + 39 + 67 + 7 + 1 = 147.\n",
      "\n",
      "The even numbers in the group are: 32, 82, 48, 26.\n",
      "The sum of even numbers = 32 + 82 + 48 + 26 = 188.\n",
      "Twice the sum of even numbers = 2 * 188 = 376.\n",
      "\n",
      "The difference between the sum of odd numbers and twice the sum of even numbers is: 147 - 376 = -229.\n",
      "\n",
      "Since -229 is an odd number, the statement is true.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced few-shot example for a more complex reasoning task\n",
    "few_shot_messages_complex_reasoning = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a model that solves complex numerical reasoning problems.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"The sum of odd numbers in this group minus the sum of even numbers results in an odd number: 4, 8, 9, 15, 12, 2, 1. The answer is False.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"The sum of odd numbers in this group minus the sum of even numbers results in an odd number: 17, 10, 19, 4, 8, 12, 24. The answer is True.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"The sum of odd numbers in this group minus twice the sum of even numbers results in an odd number: 15, 32, 5, 13, 82, 39, 67, 7, 1, 48, 26. The answer is?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the function with few-shot prompting for complex reasoning\n",
    "few_shot_response_complex_reasoning = chat_completion_request(messages=few_shot_messages_complex_reasoning)\n",
    "print(few_shot_response_complex_reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137bb32-d297-4c81-b25c-91b7c681f1b9",
   "metadata": {},
   "source": [
    "## Tips for Effective Few-Shot Prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11055e6d-f4d4-4e3b-a88e-641308e5ff1d",
   "metadata": {},
   "source": [
    "### Distribution of Input Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5270b55-daad-4929-9253-f49d1c4ebba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 16:33:40,101 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that classifies animals into categories: Mammal or Bird.'}, {'role': 'user', 'content': 'Classify the following animal: Kangaroo.'}, {'role': 'assistant', 'content': 'Kangaroo is a Mammal.'}, {'role': 'user', 'content': 'Classify the following animal: Elephant.'}, {'role': 'assistant', 'content': 'Elephant is a Mammal.'}, {'role': 'user', 'content': 'Classify the following animal: Penguin.'}, {'role': 'assistant', 'content': 'Penguin is a Bird.'}, {'role': 'user', 'content': 'Classify the following animal: Eagle.'}, {'role': 'assistant', 'content': 'Eagle is a Bird.'}, {'role': 'user', 'content': 'Classify the following animal: Hippopotomus.'}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-19 16:33:40,106 [_trace.py:trace:45] DEBUG - close.started\n",
      "2024-01-19 16:33:40,119 [_trace.py:trace:45] DEBUG - close.complete\n",
      "2024-01-19 16:33:40,129 [_trace.py:trace:45] DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2024-01-19 16:33:40,180 [_trace.py:trace:45] DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10720a230>\n",
      "2024-01-19 16:33:40,180 [_trace.py:trace:45] DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1073446c0> server_hostname='api.openai.com' timeout=5.0\n",
      "2024-01-19 16:33:40,213 [_trace.py:trace:45] DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10720ba90>\n",
      "2024-01-19 16:33:40,214 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 16:33:40,214 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-19 16:33:40,215 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-19 16:33:40,215 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-19 16:33:40,216 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 16:33:40,498 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 19 Jan 2024 21:33:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'211'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159887'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'42ms'), (b'x-request-id', b'e0e64b26a8df9dda357fb08017471ceb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'848232874dc36730-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-19 16:33:40,499 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-19 16:33:40,501 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-19 16:33:40,503 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-19 16:33:40,504 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-19 16:33:40,504 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-19 16:33:40,506 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hippopotamus is a Mammal.\n"
     ]
    }
   ],
   "source": [
    "# few-shot example with two categories\n",
    "few_shot_messages = [\n",
    "    {\"role\": \"system\",\"content\": \"You are a model that classifies animals into categories: Mammal or Bird.\"},\n",
    "    \n",
    "    {\"role\": \"user\",\"content\": \"Classify the following animal: Kangaroo.\"},\n",
    "    {\"role\": \"assistant\",\"content\": \"Kangaroo is a Mammal.\"},\n",
    "    \n",
    "    {\"role\": \"user\",\"content\": \"Classify the following animal: Elephant.\"},\n",
    "    {\"role\": \"assistant\",\"content\": \"Elephant is a Mammal.\"},\n",
    "    \n",
    "    {\"role\": \"user\",\"content\": \"Classify the following animal: Penguin.\"},\n",
    "    {\"role\": \"assistant\",\"content\": \"Penguin is a Bird.\"},\n",
    "    \n",
    "    {\"role\": \"user\",\"content\": \"Classify the following animal: Eagle.\"},\n",
    "    {\"role\": \"assistant\",\"content\": \"Eagle is a Bird.\"},\n",
    "    \n",
    "    {\"role\": \"user\",\"content\": \"Classify the following animal: Hippopotomus.\"}\n",
    "]\n",
    "\n",
    "# Call the function with revised few-shot prompting for two categories\n",
    "category = chat_completion_request(messages=few_shot_messages)\n",
    "print(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31af65-d97b-4761-b042-f8836659da5b",
   "metadata": {},
   "source": [
    "### Consistentcy in format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1ebafd3-208c-476d-ba2c-ed240cbc576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 16:45:11,594 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that translates English sentences to French.'}, {'role': 'user', 'content': \"Translate: 'Hello, how are you?'\"}, {'role': 'assistant', 'content': 'Bonjour, comment ça va?'}, {'role': 'user', 'content': \"Translate: 'What is your name?'\"}, {'role': 'assistant', 'content': 'Comment vous appelez-vous?'}, {'role': 'user', 'content': \"Translate: 'I am learning French.'\"}, {'role': 'assistant', 'content': \"J'apprends le français.\"}, {'role': 'user', 'content': \"Translate: 'This is a beautiful day.'\"}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-19 16:45:11,602 [_trace.py:trace:45] DEBUG - close.started\n",
      "2024-01-19 16:45:11,603 [_trace.py:trace:45] DEBUG - close.complete\n",
      "2024-01-19 16:45:11,604 [_trace.py:trace:45] DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2024-01-19 16:45:11,654 [_trace.py:trace:45] DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1073596f0>\n",
      "2024-01-19 16:45:11,659 [_trace.py:trace:45] DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1073446c0> server_hostname='api.openai.com' timeout=5.0\n",
      "2024-01-19 16:45:11,698 [_trace.py:trace:45] DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1073723e0>\n",
      "2024-01-19 16:45:11,699 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 16:45:11,702 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-19 16:45:11,703 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-19 16:45:11,705 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-19 16:45:11,706 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 16:45:11,961 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 19 Jan 2024 21:45:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'192'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159909'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'34ms'), (b'x-request-id', b'a3a64fb4100078e341588231ec122600'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'848243693d39453d-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-19 16:45:11,962 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-19 16:45:11,963 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-19 16:45:11,965 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-19 16:45:11,966 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-19 16:45:11,967 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-19 16:45:11,968 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est une belle journée.\n"
     ]
    }
   ],
   "source": [
    "# Few-shot example with consistent format for translation\n",
    "consistent_format_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that translates English sentences to French.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"Translate: 'Hello, how are you?'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Bonjour, comment ça va?\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"Translate: 'What is your name?'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Comment vous appelez-vous?\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"Translate: 'I am learning French.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"J'apprends le français.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"Translate: 'This is a beautiful day.'\"}\n",
    "]\n",
    "\n",
    "# Call the function with consistent format for translation\n",
    "consistent_translation = chat_completion_request(messages=consistent_format_messages)\n",
    "print(consistent_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8a9678e-a727-4723-bb5a-a00437887e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 16:45:15,060 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that translates English sentences to French.'}, {'role': 'user', 'content': \"How do you say in French: 'Good morning'?\"}, {'role': 'assistant', 'content': 'Bonjour.'}, {'role': 'user', 'content': \"Translate this: 'I like to travel.'\"}, {'role': 'assistant', 'content': \"J'aime voyager.\"}, {'role': 'user', 'content': \"'It's raining today' in French is?\"}, {'role': 'assistant', 'content': \"Il pleut aujourd'hui.\"}, {'role': 'user', 'content': \"How would you translate 'Where is the nearest station?' to French?\"}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-19 16:45:15,062 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 16:45:15,063 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-19 16:45:15,064 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-19 16:45:15,064 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-19 16:45:15,065 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 16:45:15,452 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 19 Jan 2024 21:45:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'255'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159906'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'35ms'), (b'x-request-id', b'ebf673aa9cce8b35ca4646629cf62064'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8482437e48c2453d-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-19 16:45:15,453 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-19 16:45:15,454 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-19 16:45:15,456 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-19 16:45:15,457 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-19 16:45:15,458 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-19 16:45:15,458 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Où se trouve la station la plus proche ?\n"
     ]
    }
   ],
   "source": [
    "# Few-shot example with inconsistent format for translation\n",
    "inconsistent_format_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that translates English sentences to French.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"How do you say in French: 'Good morning'?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Bonjour.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"Translate this: 'I like to travel.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"J'aime voyager.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"'It's raining today' in French is?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Il pleut aujourd'hui.\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"How would you translate 'Where is the nearest station?' to French?\"}\n",
    "]\n",
    "\n",
    "# Call the function with inconsistent format for translation\n",
    "inconsistent_translation = chat_completion_request(messages=inconsistent_format_messages)\n",
    "print(inconsistent_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fffd9e-b06e-4d5a-85f6-5ba002472791",
   "metadata": {},
   "source": [
    "### True Distribution of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c825d4c5-68b0-49de-b6ec-8f883efd1cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 16:48:15,934 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that categorizes news articles into topics: Politics, Sports, Technology, or Entertainment.'}, {'role': 'user', 'content': \"Categorize this article: 'Government announces new environmental policy.'\"}, {'role': 'assistant', 'content': 'Politics'}, {'role': 'user', 'content': \"Categorize this article: 'Elections results show surprising turn of events.'\"}, {'role': 'assistant', 'content': 'Politics'}, {'role': 'user', 'content': \"Categorize this article: 'Local team wins championship after dramatic final.'\"}, {'role': 'assistant', 'content': 'Sports'}, {'role': 'user', 'content': \"Categorize this article: 'New smartphone model features the latest in AI technology.'\"}, {'role': 'assistant', 'content': 'Technology'}, {'role': 'user', 'content': \"Categorize this article: 'Famous actor stars in a new blockbuster movie.'\"}, {'role': 'assistant', 'content': 'Entertainment'}, {'role': 'user', 'content': \"Categorize this article: 'Breakthrough in Renewable Energy Technology Unveiled at Global Conference.'\"}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-19 16:48:15,937 [_trace.py:trace:45] DEBUG - close.started\n",
      "2024-01-19 16:48:15,938 [_trace.py:trace:45] DEBUG - close.complete\n",
      "2024-01-19 16:48:15,938 [_trace.py:trace:45] DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2024-01-19 16:48:15,987 [_trace.py:trace:45] DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x107371ae0>\n",
      "2024-01-19 16:48:15,995 [_trace.py:trace:45] DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1073446c0> server_hostname='api.openai.com' timeout=5.0\n",
      "2024-01-19 16:48:16,042 [_trace.py:trace:45] DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x107370df0>\n",
      "2024-01-19 16:48:16,049 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 16:48:16,057 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-19 16:48:16,057 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-19 16:48:16,059 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-19 16:48:16,062 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 16:48:16,252 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 19 Jan 2024 21:48:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'87'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159812'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'70ms'), (b'x-request-id', b'd438670a6a5490831c674778584638ff'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'848247e8df1a44d1-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-19 16:48:16,264 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-19 16:48:16,276 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-19 16:48:16,294 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-19 16:48:16,295 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-19 16:48:16,297 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-19 16:48:16,303 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology\n"
     ]
    }
   ],
   "source": [
    "# Final example with true distribution of labels for categorizing news articles\n",
    "distribution_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that categorizes news articles into topics: Politics, Sports, Technology, or Entertainment.\"},\n",
    "    \n",
    "    # 40% Politics\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'Government announces new environmental policy.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Politics\"},\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'Elections results show surprising turn of events.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Politics\"},\n",
    "    \n",
    "    # 30% Sports\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'Local team wins championship after dramatic final.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sports\"},\n",
    "    \n",
    "    # 20% Technology\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'New smartphone model features the latest in AI technology.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Technology\"},\n",
    "    \n",
    "    # 10% Entertainment\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'Famous actor stars in a new blockbuster movie.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Entertainment\"},\n",
    "    \n",
    "    # New article to categorize\n",
    "    {\"role\": \"user\", \"content\": \"Categorize this article: 'Breakthrough in Renewable Energy Technology Unveiled at Global Conference.'\"}\n",
    "]\n",
    "\n",
    "# Call the function with final distribution of labels for categorizing news articles\n",
    "categorization = chat_completion_request(messages=distribution_messages)\n",
    "print(categorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25e5a7-8d03-4ba4-9376-50fe2bfa41b1",
   "metadata": {},
   "source": [
    "# Lesson 3: Lost in the Middle\n",
    "\n",
    "- is a phenomenon where the model pays more attention to the beginning and end of a text, while important details in the middle may be overlooked or given less emphasis.\n",
    "- typically occurs in long texts or complex prompts, leading to incomplete or inaccurate processing of the full content\n",
    "- somewhat similar to a person skimming through a long article and mainly remembering the introduction and conclusion, while missing key points in the middle\n",
    "- To counter this, breaking down tasks into smaller and add more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b57a25bf-835f-4f48-b4f2-8178e076a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 17:14:47,808 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that helps plan travel itineraries.'}, {'role': 'user', 'content': 'Create a week-long travel itinerary for a trip to Japan, starting from March 1, 2024. Include a day-by-day schedule covering Tokyo, Kyoto, and Osaka. For each city, recommend unique, non-touristy restaurants and hidden gems, specifying dishes that are both traditional and allergy-friendly. Include cultural events happening during the week, particularly those that allow for active participation. Suggest accommodations that are eco-friendly, budget-conscious, and offer traditional experiences. Detail transportation options between cities and within each city, prioritizing eco-friendly choices and cost-effectiveness. Each day should have outdoor activities, historical sites, shopping in local markets, and relaxation spots like onsens. Also, provide weather advice for each day, language tips, and essential cultural etiquette to follow. Ensure the plan caters to a family with young children, considering accessibility and family-friendly activities.'}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-19 17:14:47,810 [_trace.py:trace:45] DEBUG - close.started\n",
      "2024-01-19 17:14:47,811 [_trace.py:trace:45] DEBUG - close.complete\n",
      "2024-01-19 17:14:47,812 [_trace.py:trace:45] DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2024-01-19 17:14:47,882 [_trace.py:trace:45] DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10735bfa0>\n",
      "2024-01-19 17:14:47,883 [_trace.py:trace:45] DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1073446c0> server_hostname='api.openai.com' timeout=5.0\n",
      "2024-01-19 17:14:47,936 [_trace.py:trace:45] DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10735a320>\n",
      "2024-01-19 17:14:47,937 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 17:14:47,938 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-19 17:14:47,939 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-19 17:14:47,941 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-19 17:14:47,943 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 17:15:09,611 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 19 Jan 2024 22:15:09 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'21249'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159730'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'101ms'), (b'x-request-id', b'468c5aef005fbd46905e364dc2addcd1'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84826ec5fe93453d-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-19 17:15:09,619 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-19 17:15:09,622 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-19 17:15:09,636 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-19 17:15:09,636 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-19 17:15:09,637 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-19 17:15:09,638 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1: Tokyo\n",
      "- Arrive in Tokyo and check into your eco-friendly accommodation, such as the Andon Ryokan, which offers traditional Japanese-style rooms.\n",
      "- Start your day with a visit to Ueno Park, where you can enjoy cherry blossoms if they are in season. Take a stroll around Shinobazu Pond and visit the Tokyo National Museum.\n",
      "- For lunch, head to Kanda Yabu Soba, a hidden gem known for its delicious buckwheat noodles. They offer gluten-free options for those with allergies.\n",
      "- In the afternoon, explore the historic district of Asakusa. Visit Senso-ji Temple and Nakamise Shopping Street, where you can find traditional souvenirs.\n",
      "- Enjoy a relaxing evening at Odaiba Seaside Park, where you can take in the beautiful views of Tokyo Bay and visit the Odaiba Onsen Monogatari for a traditional hot spring experience.\n",
      "- Weather advice: March in Tokyo can be chilly, so dress in layers and bring a jacket. Check the weather forecast for possible rain.\n",
      "\n",
      "Day 2: Tokyo\n",
      "- Start your day with a visit to Tsukiji Fish Market, where you can witness the bustling seafood auction and enjoy fresh sushi at one of the local stalls.\n",
      "- After breakfast, head to the teamLab Borderless digital art museum in Odaiba. It's a unique and interactive experience that both kids and adults will love.\n",
      "- For lunch, try Tofuya Ukai, a traditional tofu restaurant hidden in a beautiful garden. They offer allergy-friendly options and a serene atmosphere.\n",
      "- In the afternoon, explore the trendy neighborhood of Harajuku. Visit Takeshita Street for quirky shops and street food, and then stroll through the peaceful Meiji Shrine.\n",
      "- End your day with a visit to Shibuya Crossing, one of the busiest intersections in the world. Grab dinner at Nonbei Yokocho, a small alley with traditional izakayas.\n",
      "- Weather advice: March weather in Tokyo can still be chilly, so dress warmly and bring an umbrella in case of rain.\n",
      "\n",
      "Day 3: Kyoto\n",
      "- Take a morning bullet train from Tokyo to Kyoto, which is an exciting experience for kids. Check into your eco-friendly accommodation, such as the Green Rich Hotel Kyoto Ekimae.\n",
      "- Start your day with a visit to Kiyomizu-dera Temple, a UNESCO World Heritage site known for its wooden terrace and panoramic views of Kyoto.\n",
      "- For lunch, try Giro Giro Hitoshina, a hidden gem serving kaiseki cuisine with allergy-friendly options. The dishes are beautifully presented and showcase Kyoto's seasonal ingredients.\n",
      "- After lunch, explore the historic district of Higashiyama. Take a stroll along the picturesque streets, visit Yasaka Shrine, and explore the shops along Sannen-zaka and Ninen-zaka.\n",
      "- In the afternoon, participate in a tea ceremony at En Tea House, where you can learn about the art of tea preparation and enjoy matcha tea and traditional sweets.\n",
      "- Weather advice: March in Kyoto can still be chilly, so dress warmly and bring a light jacket. Check the weather forecast for possible rain.\n",
      "\n",
      "Day 4: Kyoto\n",
      "- Start your day with a visit to Arashiyama Bamboo Grove, a serene bamboo forest that offers a unique photo opportunity. Explore the nearby Tenryu-ji Temple and its beautiful gardens.\n",
      "- For lunch, try Shoraian, a hidden gem restaurant in Arashiyama that offers allergy-friendly options. They specialize in tofu dishes and have a lovely view of the surrounding mountains.\n",
      "- After lunch, take a scenic boat ride on the Hozu River, enjoying the beautiful landscapes of Kyoto's outskirts.\n",
      "- In the afternoon, visit Kinkaku-ji Temple, also known as the Golden Pavilion, and marvel at its stunning golden exterior and reflection in the pond.\n",
      "- End your day with a visit to Nishiki Market, Kyoto's famous food market. Sample local delicacies and pick up some souvenirs.\n",
      "- Weather advice: March weather in Kyoto can still be chilly, so dress warmly and bring an umbrella in case of rain.\n",
      "\n",
      "Day 5: Osaka\n",
      "- Take a morning train from Kyoto to Osaka. Check into your eco-friendly accommodation, such as the Hotel Monterey Grasmere Osaka.\n",
      "- Start your day with a visit to Osaka Castle, a magnificent historical landmark. Explore the castle grounds and enjoy the panoramic views from the top.\n",
      "- For lunch, try Ajinoya Okonomiyaki, a local favorite for okonomiyaki, a savory pancake filled with various ingredients. They offer gluten-free options for those with allergies.\n",
      "- After lunch, visit the vibrant district of Dotonbori. Explore the bustling streets, try local street food, and take a boat ride along the Dotonbori River.\n",
      "- In the afternoon, visit the Osaka Aquarium Kaiyukan, one of the largest aquariums in the world. It offers interactive exhibits and a variety of marine life.\n",
      "- Weather advice: March in Osaka can still be chilly, so dress warmly and bring a light jacket. Check the weather forecast for possible rain.\n",
      "\n",
      "Day 6: Osaka\n",
      "- Start your day with a visit to Universal Studios Japan. Enjoy the thrilling rides, shows, and attractions inspired by popular movies and characters.\n",
      "- For lunch, try Kushikatsu Daruma, a local spot famous for its kushikatsu, deep-fried skewers. They have allergy-friendly options and a family-friendly atmosphere.\n",
      "- In the afternoon, visit Osaka Museum of Housing and Living, where you can experience life in Osaka during different historical periods.\n",
      "- Take a leisurely walk along the picturesque Osaka Castle Park, enjoying the cherry blossoms if they are in bloom.\n",
      "- End your day with a visit to Spa World, a unique onsen complex that offers various themed hot spring baths from around the world.\n",
      "- Weather advice: March weather in Osaka can still be chilly, so dress warmly and bring an umbrella in case of rain.\n",
      "\n",
      "Day 7: Tokyo\n",
      "- Take a morning bullet train from Osaka back to Tokyo.\n",
      "- Spend your last day in Tokyo exploring the trendy neighborhood of Shimokitazawa. Browse through vintage shops, enjoy street performances, and try local treats.\n",
      "- For lunch, try Hanbey Fuunji, a hidden gem ramen shop known for its allergy-friendly options. They offer a delicious and creamy tsukemen, a dipping-style ramen dish.\n",
      "- In the afternoon, visit the teamLab Planets digital art museum for another immersive and interactive experience.\n",
      "- End your trip with a visit to Tokyo Skytree, the tallest tower in Japan. Enjoy the panoramic views of Tokyo and have a farewell dinner at one of the restaurants in the complex.\n",
      "- Weather advice: March weather in Tokyo can still be chilly, so dress warmly and bring a light jacket. Check the weather forecast for possible rain.\n",
      "\n",
      "Language tips:\n",
      "- Learn a few basic Japanese phrases such as \"hello\" (konnichiwa), \"thank you\" (arigatou gozaimasu), and \"excuse me\" (sumimasen).\n",
      "- Carry a pocket-sized English-Japanese phrasebook for assistance.\n",
      "\n",
      "Cultural etiquette:\n",
      "- Remember to bow when greeting or thanking someone. A slight bow is appropriate in most situations.\n",
      "- Remove your shoes when entering traditional establishments, such as ryokans and some restaurants.\n",
      "- Avoid speaking loudly or causing disruptions in public places.\n",
      "- Always have cash on hand, as some smaller establishments may not accept credit cards.\n",
      "\n",
      "Note: It is important to check for any travel advisories, entry requirements, and event availability closer to your travel dates, as circumstances may change.\n"
     ]
    }
   ],
   "source": [
    "# Example showing an overloaded task for creating a detailed travel itinerary\n",
    "overloaded_itinerary_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that helps plan travel itineraries.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Create a week-long travel itinerary for a trip to Japan, starting from March 1, 2024. \"\n",
    "        \"Include a day-by-day schedule covering Tokyo, Kyoto, and Osaka. \"\n",
    "        \"For each city, recommend unique, non-touristy restaurants and hidden gems, \"\n",
    "        \"specifying dishes that are both traditional and allergy-friendly. \"\n",
    "        \"Include cultural events happening during the week, particularly those that \"\n",
    "        \"allow for active participation. Suggest accommodations that are eco-friendly, \"\n",
    "        \"budget-conscious, and offer traditional experiences. Detail transportation options \"\n",
    "        \"between cities and within each city, prioritizing eco-friendly choices and cost-effectiveness. \"\n",
    "        \"Each day should have outdoor activities, historical sites, shopping in local markets, \"\n",
    "        \"and relaxation spots like onsens. Also, provide weather advice for each day, language tips, \"\n",
    "        \"and essential cultural etiquette to follow. Ensure the plan caters to a family with young children, \"\n",
    "        \"considering accessibility and family-friendly activities.\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "# Call the function with the overloaded itinerary task\n",
    "overloaded_itinerary_response = chat_completion_request(messages=overloaded_itinerary_request)\n",
    "print(overloaded_itinerary_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87b0e4c1-a5f1-41d6-b2d1-abbd6cab9e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 17:17:51,254 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that helps write articles.'}, {'role': 'user', 'content': 'Write a complete article about the impact of climate change on global agriculture. Include an introduction, the effects on different types of crops, how farmers are adapting, the role of technology in addressing these challenges, and a conclusion.'}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-19 17:17:51,263 [_trace.py:trace:45] DEBUG - close.started\n",
      "2024-01-19 17:17:51,265 [_trace.py:trace:45] DEBUG - close.complete\n",
      "2024-01-19 17:17:51,266 [_trace.py:trace:45] DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2024-01-19 17:17:51,314 [_trace.py:trace:45] DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x107363dc0>\n",
      "2024-01-19 17:17:51,317 [_trace.py:trace:45] DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1073446c0> server_hostname='api.openai.com' timeout=5.0\n",
      "2024-01-19 17:17:51,366 [_trace.py:trace:45] DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x106eb84f0>\n",
      "2024-01-19 17:17:51,369 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 17:17:51,371 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-19 17:17:51,372 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-19 17:17:51,374 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-19 17:17:51,375 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:00,507 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 19 Jan 2024 22:18:00 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'8970'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159909'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'34ms'), (b'x-request-id', b'a30e9f07800bf56aa54b7aada1d932d9'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'848273407dddb0ac-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-19 17:18:00,512 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-19 17:18:00,514 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:00,517 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-19 17:18:00,518 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-19 17:18:00,519 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-19 17:18:00,520 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Climate Change and Global Agriculture: Adapting to a Warming World\n",
      "\n",
      "Introduction:\n",
      "Climate change has emerged as one of the most pressing challenges of our time, with far-reaching impacts on various sectors. Among the most vulnerable is global agriculture, which not only provides sustenance for a growing population but also plays a vital role in economies worldwide. The effects of climate change on agriculture are profound, affecting different types of crops, necessitating adaptation strategies from farmers, and highlighting the crucial role of technology in addressing these challenges.\n",
      "\n",
      "Effects on Different Types of Crops:\n",
      "Climate change disrupts the delicate balance of temperature, precipitation, and seasonal patterns that crops rely on for optimal growth. Rising temperatures, shifting rainfall patterns, and extreme weather events pose significant threats to agricultural productivity. Cereal crops, such as wheat, rice, and corn, are particularly vulnerable. Higher temperatures reduce crop yields and quality, while erratic rainfall patterns lead to droughts or floods, impacting crop growth, water availability, and soil fertility.\n",
      "\n",
      "Specialized crops, such as coffee, cocoa, and tea, are also at risk due to changing climate conditions. These crops are highly sensitive to temperature and rainfall variations, making them susceptible to pests, diseases, and reduced yields. Additionally, fruit and vegetable crops face challenges like altered pollination patterns, increased pest pressure, and reduced nutritional value due to changing climate conditions.\n",
      "\n",
      "Adaptation Strategies for Farmers:\n",
      "Farmers across the globe are recognizing the need to adapt their agricultural practices to mitigate the impacts of climate change. They are implementing various strategies to enhance resilience and maintain productivity. These include altering planting dates, using drought-tolerant and heat-resistant crop varieties, adopting precision farming techniques, and implementing improved water management practices.\n",
      "\n",
      "Crop diversification is another crucial strategy employed by farmers to reduce risks associated with climate change. By growing a variety of crops, farmers can spread their risks and adapt to changing conditions. Additionally, agroforestry practices, such as integrating trees with crops, help improve soil health, conserve water, and provide shade, offering a buffer against extreme weather events.\n",
      "\n",
      "Role of Technology in Addressing Challenges:\n",
      "Technology plays a pivotal role in helping farmers tackle the challenges posed by climate change. Precision agriculture techniques, such as satellite imagery, drones, and sensor-based systems, enable farmers to monitor crop health, optimize resource use, and make informed decisions. These technologies provide real-time data on soil moisture, nutrient levels, and pest infestations, allowing farmers to take timely action.\n",
      "\n",
      "Genetic engineering and breeding programs are developing crop varieties with enhanced resilience to withstand changing climate conditions. Scientists are working to develop heat-tolerant, drought-resistant, and disease-resistant crop varieties that can thrive in adverse environments. Additionally, innovative irrigation systems, such as drip irrigation and precision sprinklers, help conserve water and improve water-use efficiency.\n",
      "\n",
      "Conclusion:\n",
      "Climate change poses significant challenges to global agriculture, threatening food security and livelihoods worldwide. The impacts on different types of crops necessitate adaptive measures from farmers, who are actively implementing strategies to enhance resilience. Technology, ranging from precision agriculture techniques to genetic engineering, plays a crucial role in addressing these challenges by providing tools and innovations to mitigate the impacts of climate change.\n",
      "\n",
      "However, collective global action is essential to combat climate change effectively. Governments, organizations, and individuals must work together to reduce greenhouse gas emissions, promote sustainable farming practices, and support farmers in adopting climate-smart strategies. By prioritizing climate resilience in agriculture, we can safeguard our food systems and ensure a sustainable future for generations to come.\n"
     ]
    }
   ],
   "source": [
    "# Bad Example: Asking for an Entire Article in One Prompt\n",
    "full_article_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that helps write articles.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Write a complete article about the impact of climate change on global agriculture. \"\n",
    "        \"Include an introduction, the effects on different types of crops, how farmers are adapting, \"\n",
    "        \"the role of technology in addressing these challenges, and a conclusion.\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "# Call the function with the full article request\n",
    "full_article_response = chat_completion_request(messages=full_article_request)\n",
    "print(full_article_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "664af572-9bcd-4c20-a776-401b11788fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 17:18:13,443 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that helps write article outlines.'}, {'role': 'user', 'content': 'Create an outline for an article about the impact of climate change on global agriculture.'}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-19 17:18:13,452 [_trace.py:trace:45] DEBUG - close.started\n",
      "2024-01-19 17:18:13,455 [_trace.py:trace:45] DEBUG - close.complete\n",
      "2024-01-19 17:18:13,456 [_trace.py:trace:45] DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2024-01-19 17:18:13,509 [_trace.py:trace:45] DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10720b940>\n",
      "2024-01-19 17:18:13,512 [_trace.py:trace:45] DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1073446c0> server_hostname='api.openai.com' timeout=5.0\n",
      "2024-01-19 17:18:13,597 [_trace.py:trace:45] DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x107358130>\n",
      "2024-01-19 17:18:13,599 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:13,600 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-19 17:18:13,601 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:13,602 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-19 17:18:13,602 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:20,701 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 19 Jan 2024 22:18:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'6729'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159947'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'19ms'), (b'x-request-id', b'791092ea1fa1aac85516fdc786998e78'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'848273cb6fcd7bc3-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-19 17:18:20,707 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-19 17:18:20,709 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:20,711 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-19 17:18:20,712 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-19 17:18:20,713 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-19 17:18:20,728 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
      "2024-01-19 17:18:20,759 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that helps write article sections.'}, {'role': 'user', 'content': 'Write the introduction for the article based on the provided outline.'}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-19 17:18:20,761 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:20,763 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-19 17:18:20,763 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:20,764 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-19 17:18:20,764 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I. Introduction\n",
      "    A. Definition of climate change\n",
      "    B. Importance of agriculture in global food production\n",
      "    C. Thesis statement: Climate change poses significant challenges to global agriculture and food security.\n",
      "\n",
      "II. Climate Change and its Causes\n",
      "    A. Explanation of greenhouse gases and their role in climate change\n",
      "    B. Human activities contributing to climate change\n",
      "    C. Natural factors influencing climate patterns\n",
      "\n",
      "III. Effects of Climate Change on Agriculture\n",
      "    A. Changes in temperature and precipitation patterns\n",
      "        1. Increased frequency and intensity of extreme weather events\n",
      "        2. Droughts and water scarcity\n",
      "        3. Floods and waterlogging\n",
      "    B. Impact on crop production\n",
      "        1. Shifts in suitable growing regions\n",
      "        2. Changes in crop yields and quality\n",
      "        3. Increased vulnerability to pests and diseases\n",
      "    C. Effects on livestock and fisheries\n",
      "        1. Heat stress and reduced productivity in livestock\n",
      "        2. Altered distribution and abundance of fish species\n",
      "\n",
      "IV. Implications for Food Security\n",
      "    A. Decreased availability and affordability of food\n",
      "    B. Increased risk of malnutrition and food-related health issues\n",
      "    C. Disruption of global food trade and supply chains\n",
      "\n",
      "V. Adaptation and Mitigation Strategies\n",
      "    A. Improving agricultural practices and technologies\n",
      "        1. Crop diversification and breeding climate-resilient varieties\n",
      "        2. Efficient water management and irrigation techniques\n",
      "        3. Precision agriculture and digital tools\n",
      "    B. Enhancing ecosystem resilience and biodiversity\n",
      "        1. Conservation agriculture and agroforestry\n",
      "        2. Soil health management and carbon sequestration\n",
      "        3. Protecting pollinators and natural pest control\n",
      "    C. Policy interventions and international cooperation\n",
      "        1. Climate change adaptation and mitigation policies\n",
      "        2. Financial support for farmers in vulnerable regions\n",
      "        3. Collaboration between governments, scientists, and stakeholders\n",
      "\n",
      "VI. Case Studies and Success Stories\n",
      "    A. Examples of countries implementing climate-smart agriculture\n",
      "    B. Innovative solutions and practices adopted by farmers\n",
      "    C. Positive outcomes and lessons learned\n",
      "\n",
      "VII. Conclusion\n",
      "    A. Recap of the impact of climate change on global agriculture\n",
      "    B. Call to action for individuals, governments, and organizations\n",
      "    C. Importance of sustainable agriculture in mitigating climate change and ensuring food security.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 17:18:25,386 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 19 Jan 2024 22:18:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'4246'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159837'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'60ms'), (b'x-request-id', b'c855809eda4c321f9a8c1b42d3ca602e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'848273f82cfb7bc3-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-19 17:18:25,387 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-19 17:18:25,388 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:25,389 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-19 17:18:25,390 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-19 17:18:25,392 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-19 17:18:25,393 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
      "2024-01-19 17:18:25,407 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that helps refine articles.'}, {'role': 'user', 'content': 'Here is the complete article [insert the written article]. Can you suggest improvements?'}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-19 17:18:25,410 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:25,411 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-19 17:18:25,412 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:25,412 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-19 17:18:25,413 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Benefits of Meditation: A Path to Inner Peace and Well-being\n",
      "\n",
      "Introduction:\n",
      "\n",
      "In today's fast-paced and demanding world, finding moments of tranquility and inner peace can seem like an elusive goal. The constant noise and distractions can leave us feeling overwhelmed and disconnected from ourselves. However, a practice that has been gaining popularity in recent years offers a solution to this modern predicament – meditation.\n",
      "\n",
      "Meditation is an ancient practice that has been used for thousands of years to cultivate a sense of calm, clarity, and self-awareness. It involves training the mind to focus and redirect thoughts, ultimately leading to a state of deep relaxation and inner peace. While meditation has its roots in various religious and spiritual traditions, it has transcended these boundaries and is now embraced by people from all walks of life, regardless of their beliefs or backgrounds.\n",
      "\n",
      "The benefits of meditation extend far beyond just moments of tranquility. Scientific research has shown that regular meditation practice can have a profound impact on our physical, mental, and emotional well-being. From reducing stress and anxiety to improving concentration and emotional resilience, the positive effects of meditation are wide-ranging and significant.\n",
      "\n",
      "In this article, we will explore the numerous benefits of meditation and how it can enhance our overall quality of life. We will delve into the scientific evidence supporting these claims and provide practical tips on how to incorporate meditation into our daily routines. Whether you are a beginner or have some experience with meditation, this article aims to inspire and guide you on your journey to finding inner peace and well-being through the transformative power of meditation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 17:18:25,794 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 19 Jan 2024 22:18:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'298'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159949'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'19ms'), (b'x-request-id', b'f16f9c9d56a28025f0d77dfb10f421bf'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'848274153c747bc3-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-19 17:18:25,796 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-19 17:18:25,796 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-19 17:18:25,797 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-19 17:18:25,798 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-19 17:18:25,799 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-19 17:18:25,800 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Please provide me with the article, and I'll be happy to suggest improvements.\n"
     ]
    }
   ],
   "source": [
    "# Good example: Breaking down the article writing process\n",
    "\n",
    "# Step 1: Asking for an outline\n",
    "outline_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that helps write article outlines.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Create an outline for an article about the impact of climate change on global agriculture.\"}\n",
    "]\n",
    "\n",
    "# Call the function for the outline\n",
    "outline_response = chat_completion_request(messages=outline_request)\n",
    "print(outline_response)\n",
    "\n",
    "# Assuming the outline is received, proceed with writing sections\n",
    "\n",
    "# Step 2: Writing individual sections (Example: Writing the introduction)\n",
    "introduction_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that helps write article sections.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write the introduction for the article based on the provided outline.\"}\n",
    "]\n",
    "\n",
    "# Call the function for the introduction section\n",
    "introduction_response = chat_completion_request(messages=introduction_request)\n",
    "print(introduction_response)\n",
    "\n",
    "# Similarly, continue with other sections...\n",
    "\n",
    "# Step 3: Refining the entire article\n",
    "# This step is done after all sections are written\n",
    "article_refinement_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that helps refine articles.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Here is the complete article [insert the written article]. Can you suggest improvements?\"}\n",
    "]\n",
    "\n",
    "# Call the function for refining the article\n",
    "article_refinement_response = chat_completion_request(messages=article_refinement_request)\n",
    "print(article_refinement_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d01abb-ba8a-47a5-b9f7-ec2e380f311b",
   "metadata": {},
   "source": [
    "# Lesson 4: Markdown Tricks\n",
    "\n",
    "- Markdown Basics: Markdown is a lightweight markup language that allows for easy text formatting using plain text syntax. It's widely used for creating formatted text on the web.\n",
    "\n",
    "- Simplicity and Readability: One of the key features of Markdown is its simplicity. The syntax is designed to be readable and straightforward, making it easy to write and read even in its raw form.\n",
    "\n",
    "## In short:\n",
    "- Use Markdown to create a clear and concise structure for your prompts.\n",
    "- Use Markdown to highlight important information in your prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c1932d9-b62c-49f3-839b-14a5987011d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Here are the markdown indicators for ChatGPT :\n",
    "#     1. # Heading 1\n",
    "#     2. ## Heading 2\n",
    "#     3. ### Heading 3\n",
    "#     4. *italic text*, **bold text**, __underlined text__\n",
    "#     5. [Hyperlink text](url)\n",
    "#     6. ![Image name](image url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c87247ba-ad80-4faf-9b0d-320a22e14b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 07:43:23,724 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that understands and organizes tasks using Markdown.'}, {'role': 'user', 'content': \"Here's a list of tasks I need you to perform. Please pay **special attention** to the deadlines:\\n\\n* **Research Task**: Compile a list of the top 5 AI trends in 2024. *Due: Next Monday*\\n* **Writing Task**: Write a summary of each trend, emphasizing their impact on healthcare. **Ensure accuracy** in data. *Due: Wednesday*\\n* **Presentation Task**: Create a slide deck for the AI trends with appropriate visuals. Remember to make it **engaging and informative**. *Due: Friday*\\n* **Feedback Session**: Organize a team meeting to discuss the slide deck. Ensure to invite all project members. *Due: Next Friday*\\n\\n__Note__: The quality of research and clarity in the presentation are crucial. Please prioritize these aspects.\"}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-20 07:43:23,737 [_trace.py:trace:45] DEBUG - close.started\n",
      "2024-01-20 07:43:23,768 [_trace.py:trace:45] DEBUG - close.complete\n",
      "2024-01-20 07:43:23,771 [_trace.py:trace:45] DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2024-01-20 07:43:23,831 [_trace.py:trace:45] DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x107208f40>\n",
      "2024-01-20 07:43:23,839 [_trace.py:trace:45] DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1073446c0> server_hostname='api.openai.com' timeout=5.0\n",
      "2024-01-20 07:43:23,881 [_trace.py:trace:45] DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x107361630>\n",
      "2024-01-20 07:43:23,882 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-20 07:43:23,883 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-20 07:43:23,884 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-20 07:43:23,884 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-20 07:43:23,885 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-01-20 07:43:26,053 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 20 Jan 2024 12:43:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'1830'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159785'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'80ms'), (b'x-request-id', b'e7dbebbe0b355d59b4476afc85184602'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=qoct3QCf.w_gpEIdEH2iNpEmS18kHwQxuj21_KEpvaE-1705754606-1-AW8c0YfEHQTYuKpkj1tImQEVu5gY9HDSscMatQkCB6C3RrdCFVTcTo4AlzQaD1uCT7fUaXRW/0rhyubmPzi6T1Q=; path=/; expires=Sat, 20-Jan-24 13:13:26 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84876722989a453f-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-20 07:43:26,056 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-20 07:43:26,059 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-20 07:43:26,062 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-20 07:43:26,063 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-20 07:43:26,063 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-20 07:43:26,064 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Task List\n",
      "\n",
      "## Research Task\n",
      "- Compile a list of the top 5 AI trends in 2024.\n",
      "- **Deadline**: Next Monday\n",
      "\n",
      "## Writing Task\n",
      "- Write a summary of each trend, emphasizing their impact on healthcare.\n",
      "- **Deadline**: Wednesday\n",
      "- **Important**: Ensure accuracy in data.\n",
      "\n",
      "## Presentation Task\n",
      "- Create a slide deck for the AI trends with appropriate visuals.\n",
      "- Make it engaging and informative.\n",
      "- **Deadline**: Friday\n",
      "- **Important**: Prioritize quality of research and clarity in the presentation.\n",
      "\n",
      "## Feedback Session\n",
      "- Organize a team meeting to discuss the slide deck.\n",
      "- Invite all project members.\n",
      "- **Deadline**: Next Friday\n"
     ]
    }
   ],
   "source": [
    "# Example of multi-task instruction with emphasis and list organization using Markdown\n",
    "multi_task_instruction_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that understands and organizes tasks using Markdown.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Here's a list of tasks I need you to perform. Please pay **special attention** to the deadlines:\\n\\n\"\n",
    "        \"* **Research Task**: Compile a list of the top 5 AI trends in 2024. *Due: Next Monday*\\n\"\n",
    "        \"* **Writing Task**: Write a summary of each trend, emphasizing their impact on healthcare. **Ensure accuracy** in data. *Due: Wednesday*\\n\"\n",
    "        \"* **Presentation Task**: Create a slide deck for the AI trends with appropriate visuals. Remember to make it **engaging and informative**. *Due: Friday*\\n\"\n",
    "        \"* **Feedback Session**: Organize a team meeting to discuss the slide deck. Ensure to invite all project members. *Due: Next Friday*\\n\\n\"\n",
    "        \"__Note__: The quality of research and clarity in the presentation are crucial. Please prioritize these aspects.\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "# Call the function to organize and emphasize the tasks\n",
    "multi_task_instruction_response = chat_completion_request(messages=multi_task_instruction_request)\n",
    "print(multi_task_instruction_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98b9e21f-0591-4902-a9c4-9324799ca960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 07:45:54,587 [_base_client.py:_build_request:433] DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a model that selects functions and formats responses in JSON.'}, {'role': 'user', 'content': \"Below is a list of functions. Read the descriptions and select the most relevant function based on the user query. Respond in JSON format. If the necessary information is not available, return an empty JSON response.\\n\\n### Available Functions:\\n\\n1. **get_order_status**: Retrieve order details including status and tracking information. *Required Input*: Order ID, Email, or Phone Number.\\n\\n2. **manage_order_change_request**: Handle modifications to an existing order. *Required Input*: Order ID, Email, or Phone Number.\\n\\n3. **get_public_info**: Access store-specific public information like policies and promotions. *Required Input*: None\\n\\n### User Query: 'I want to know when my order will arrive.'\\n\\nBased on the query, select the appropriate function and provide a JSON response.\"}], 'model': 'gpt-3.5-turbo-0613', 'function_call': None, 'functions': None, 'max_tokens': None, 'seed': 123, 'temperature': 0.5, 'top_p': 1}}\n",
      "2024-01-20 07:45:54,595 [_trace.py:trace:45] DEBUG - close.started\n",
      "2024-01-20 07:45:54,598 [_trace.py:trace:45] DEBUG - close.complete\n",
      "2024-01-20 07:45:54,600 [_trace.py:trace:45] DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2024-01-20 07:45:54,657 [_trace.py:trace:45] DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x107374e20>\n",
      "2024-01-20 07:45:54,658 [_trace.py:trace:45] DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1073446c0> server_hostname='api.openai.com' timeout=5.0\n",
      "2024-01-20 07:45:54,689 [_trace.py:trace:45] DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x107360070>\n",
      "2024-01-20 07:45:54,691 [_trace.py:trace:45] DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-01-20 07:45:54,694 [_trace.py:trace:45] DEBUG - send_request_headers.complete\n",
      "2024-01-20 07:45:54,695 [_trace.py:trace:45] DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2024-01-20 07:45:54,696 [_trace.py:trace:45] DEBUG - send_request_body.complete\n",
      "2024-01-20 07:45:54,697 [_trace.py:trace:45] DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-01-20 07:45:55,244 [_trace.py:trace:45] DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 20 Jan 2024 12:45:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'shopagain'), (b'openai-processing-ms', b'477'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159769'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'86ms'), (b'x-request-id', b'5028a55d61f9c6c73f2e7dc806713c5d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84876ad12ef312df-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-01-20 07:45:55,246 [_client.py:_send_single_request:1027] INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-01-20 07:45:55,247 [_trace.py:trace:45] DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-01-20 07:45:55,249 [_trace.py:trace:45] DEBUG - receive_response_body.complete\n",
      "2024-01-20 07:45:55,249 [_trace.py:trace:45] DEBUG - response_closed.started\n",
      "2024-01-20 07:45:55,250 [_trace.py:trace:45] DEBUG - response_closed.complete\n",
      "2024-01-20 07:45:55,251 [_base_client.py:_request:923] DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"function\": \"get_order_status\",\n",
      "  \"required_input\": \"Order ID, Email, or Phone Number\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example of selecting a function with Markdown organization and JSON response\n",
    "function_selection_request = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a model that selects functions and formats responses in JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"Below is a list of functions. Read the descriptions and select the most relevant function based on the user query. \"\n",
    "        \"Respond in JSON format. If the necessary information is not available, return an empty JSON response.\\n\\n\"\n",
    "        \n",
    "        \"### Available Functions:\\n\\n\"\n",
    "        \n",
    "        \"1. **get_order_status**: Retrieve order details including status and tracking information. \"\n",
    "        \"*Required Input*: Order ID, Email, or Phone Number.\\n\\n\"\n",
    "        \n",
    "        \"2. **manage_order_change_request**: Handle modifications to an existing order. \"\n",
    "        \"*Required Input*: Order ID, Email, or Phone Number.\\n\\n\"\n",
    "        \n",
    "        \"3. **get_public_info**: Access store-specific public information like policies and promotions. \"\n",
    "        \"*Required Input*: None\\n\\n\"\n",
    "        \n",
    "        \"### User Query: 'I want to know when my order will arrive.'\\n\\n\"\n",
    "        \"Based on the query, select the appropriate function and provide a JSON response.\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "# Call the function to select and format the response\n",
    "function_selection_response = chat_completion_request(messages=function_selection_request)\n",
    "print(function_selection_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21f26a-f761-43fc-83c8-5cbc7708baaa",
   "metadata": {},
   "source": [
    "# Lesson 5: Prompt Order Sensitivity\n",
    "\n",
    "- **Understand Prompt Order Sensitivity**: Recognize that the order in which examples are presented in a few-shot prompt can significantly impact an AI model's performance. Different orders can lead to varying levels of understanding and accuracy.\n",
    "\n",
    "- **Experiment with Different Orders**: To identify the most effective prompt order, experiment with rearranging your examples. This trial-and-error approach can reveal which sequence yields the best responses from the model.\n",
    "\n",
    "- **Start with Simple to Complex Examples**: When uncertain, a good rule of thumb is to arrange examples from the simplest to the most complex. This can help the model build its understanding progressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad6e80-1498-4e48-9d83-35d316df6807",
   "metadata": {},
   "source": [
    "### LongContextReorder in LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b008a29a-bba1-41d6-a3a6-2922211da7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Engine Response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Yes, the author met Sam Altman during the first batch of Y Combinator's Summer Founders Program. Altman would later become the second president of Y Combinator."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reorder Engine Response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Yes, the author met Sam Altman in the first batch of startups funded by Y Combinator. Altman would later become the second president of YC."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rag_llama_index_reorder import LlamaIndexQueryEngine\n",
    "\n",
    "data_dir = \"./data/paul_graham/\"\n",
    "query_engine = LlamaIndexQueryEngine(data_dir)\n",
    "\n",
    "query = \"Did the author meet Sam Altman?\"\n",
    "\n",
    "print(\"Base Engine Response:\")\n",
    "base_response = query_engine.query_base_engine(query)\n",
    "\n",
    "print(\"\\nReorder Engine Response:\")\n",
    "reorder_response = query_engine.query_reorder_engine(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85a665-ceb4-42cf-82a3-a12eff17f2b3",
   "metadata": {},
   "source": [
    "### LongContextReorder in Langchain\n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/retrievers/long_context_reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fedfd6e-22d8-48dd-9e81-f32e0665ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Engine Response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maruti/work/mercury_ml/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Yes, the author met Sam Altman. \n",
      "\n",
      "Reorder Engine Response:\n",
      "\n",
      "        Yes, the author did meet Sam Altman.\n"
     ]
    }
   ],
   "source": [
    "from rag_langchain_reorder import LangChainQueryEngine\n",
    "\n",
    "data_dir = \"./data/paul_graham/\"\n",
    "query_engine = LangChainQueryEngine(data_dir)\n",
    "\n",
    "query = \"Did the author meet Sam Altman?\"\n",
    "\n",
    "print(\"Base Engine Response:\")\n",
    "base_response = query_engine.run_query(query)\n",
    "print(base_response)\n",
    "\n",
    "print(\"\\nReorder Engine Response:\")\n",
    "reorder_response = query_engine.run_query(query, reordered=True)\n",
    "print(reorder_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee58e41-b079-4f8c-8bcb-f2dbf5eb5195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed214490-61d1-4e48-a6d0-b5d61f03e4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
