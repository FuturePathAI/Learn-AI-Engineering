{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Essential Concepts for working with LLMs\n",
    "\n",
    "1. Text Generation: Prompt writing, parameters for LLMs\n",
    "2. Retrieval & Ranking: Vectors/Embeddings - Searching for \"similar\" items\n",
    "3. Input Preprocessing: Loaders and Chunking\n",
    "4. Orchestration: Chains\n",
    "5. Model Fine-tuning: Training, Transfer Learning\n",
    "6. Output Postprocessing: Function Calling for JSON Formatting, Parsing\n",
    "7. Output Evaluation: Metrics, Scoring\n",
    "\n",
    "These are recurring concepts that are essential for working with LLMs. They are not necessarily in order of importance, but they are all important. We introduce a select few of them here and will go into more detail in the upcoming chapters. Think of this as the table of contents for the rest of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "This section is about generating text using LLMs. This is the most common use case for LLMs. We will cover how to write prompts, how to set parameters for the LLMs, and how to generate text. This is based on [How to format inputs to ChatGPT models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb) from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Chat Completion API Call\n",
    "\n",
    "A chat completion API call parameters,\n",
    "**Required**\n",
    "- `model`: the name of the model you want to use (e.g., `gpt-3.5-turbo`, `gpt-4`, `gpt-3.5-turbo-16k-1106`)\n",
    "- `messages`: a list of message objects, where each object has two required fields:\n",
    "    - `role`: the role of the messenger (either `system`, `user`, `assistant` or `tool`)\n",
    "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
    "\n",
    "Messages can also contain an optional `name` field, which give the messenger a name. E.g., `example-user`, `Alice`, `BlackbeardBot`. Names may not contain spaces.\n",
    "\n",
    "**Optional**\n",
    "- `frequency_penalty`: Penalizes tokens based on their frequency, reducing repetition.\n",
    "- `logit_bias`: Modifies likelihood of specified tokens with bias values.\n",
    "- `logprobs`: Returns log probabilities of output tokens if true.\n",
    "- `top_logprobs`: Specifies the number of most likely tokens to return at each position.\n",
    "- `max_tokens`: Sets the maximum number of generated tokens in chat completion.\n",
    "- `n`: Generates a specified number of chat completion choices for each input.\n",
    "- `presence_penalty`: Penalizes new tokens based on their presence in the text.\n",
    "- `response_format`: Specifies the output format, e.g., JSON mode.\n",
    "- `seed`: Ensures deterministic sampling with a specified seed.\n",
    "- `stop`: Specifies up to 4 sequences where the API should stop generating tokens.\n",
    "- `stream`: Sends partial message deltas as tokens become available.\n",
    "- `temperature`: Sets the sampling temperature between 0 and 2.\n",
    "- `top_p`: Uses nucleus sampling; considers tokens with top_p probability mass.\n",
    "- `tools`: Lists functions the model may call.\n",
    "- `tool_choice`: Controls the model's function calls (none/auto/function).\n",
    "- `user`: Unique identifier for end-user monitoring and abuse detection.\n",
    "\n",
    "\n",
    "As of January 2024, you can also optionally submit a list of `functions` that tell GPT whether it can generate JSON to feed into a function. For details, see the [documentation](https://platform.openai.com/docs/guides/function-calling), [API reference](https://platform.openai.com/docs/api-reference/chat), or the Cookbook guide [How to call functions with chat models](How_to_call_functions_with_chat_models.ipynb).\n",
    "\n",
    "Typically, a conversation will start with a system message that tells the assistant how to behave, followed by alternating user and assistant messages, but you are not required to follow this format.\n",
    "\n",
    "Let's look at an example chat API calls to see how the chat format works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example OpenAI Python library request\n",
    "MODEL = \"gpt-3.5-turbo-0125\"\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-9Gu0GLmeUs27ldoLlRLSVwEgUProV\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"Orange who?\",\n",
      "                \"role\": \"assistant\",\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1713815552,\n",
      "    \"model\": \"gpt-3.5-turbo-0125\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"system_fingerprint\": \"fp_c2295e73ad\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 3,\n",
      "        \"prompt_tokens\": 35,\n",
      "        \"total_tokens\": 38\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(json.loads(response.model_dump_json()), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting just the reply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Orange who?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Essentials\n",
    "\n",
    "### System Messages\n",
    "\n",
    "Unlike most other open source models, OpenAI models -- specially those after 0613, pay attention to the system messages. You can use system messages to set the overall tone for the conversation e.g. `role: system, content: \"You're a friendly assistant.\"` or `role: system, content: \"You're a friendly teaching assistant who is talking to a fifth grader with English as a second language.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In May, Nagpur experiences hot and dry weather. The temperature during this month can range from 30 to 45 degrees Celsius (86 to 113 degrees Fahrenheit). The days are usually sunny and the humidity levels are relatively low. It is advisable to stay hydrated and protect yourself from the scorching sun by wearing sunscreen, hats, and light clothing. It is also common to have occasional dust storms or thunderstorms during this time of the year.\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to be verbose and helpful\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly and helpful teaching assistant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Describe the weather in Nagpur in May\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "(print(response.choices[0].message.content),)\n",
    "print(response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Nagpur in May is typically hot and dry. Temperatures can reach highs of around 40 degrees Celsius (104 degrees Fahrenheit) during the day, with minimal rainfall. It is advisable to stay hydrated and seek shade or air-conditioned spaces to beat the heat.\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to be brief\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise, crisp assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Describe the weather in Nagpur in May\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I understand that you're frustrated, and I'm here to help. In May, Nagpur experiences hot weather with temperatures ranging from 35 to 45 degrees Celsius (95 to 113 degrees Fahrenheit). It can be quite uncomfortable during this time, so it's important to stay hydrated and take necessary precautions to beat the heat. Is there anything specific you would like assistance with regarding the weather in Nagpur?\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to be brief\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a talking to a really angry user. You need to be very helpful and patient.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Describe the weather in Nagpur in May\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr, me hearty, ye be askin' about the weather in Nagpur in May, eh? Well, let me tell ye, it be hotter than a pirate's temper after losin' his treasure! The sun be shinin' down like a fiery cannonball, makin' ye feel like ye be walkin' the plank right into a scorchin' sea of sweat. It be so hot that even the parrots be askin' for a sip of rum to cool their feathers! So, if ye be plannin' a visit to Nagpur in May, me advice be to bring yer own personal iceberg or a trusty fan made from a sail, or ye might just melt away like a puddle of melted gold doubloons! Arr, stay cool, me matey!\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to be brief\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You're talking to a pirate from the 1800 London. You need to be very funny\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Describe the weather in Nagpur in May\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
    "\n",
    "One way to show the model what you want is with faked example messages.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sudden change in direction means we don't have enough time to complete the entire project for the client.\n"
     ]
    }
   ],
   "source": [
    "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful, pattern-following assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Help me translate the following corporate jargon into plain English.\",\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Things working well together will increase revenue.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search: Retrieval & Ranking\n",
    "\n",
    "Based on [Q&A using Embeddings](https://cookbook.openai.com/examples/question_answering_using_embeddings) from OpenAI Cookbook.\n",
    "\n",
    "### Why do we even search? \n",
    "\n",
    "For every time, we need the model to use some information which it has not already seen during training. This is common when there is something new in the world, or when the model is being used in a new context e.g. your company's internal data.\n",
    "\n",
    "> GPT can learn knowledge in two ways:\n",
    ">\n",
    "> - Via model weights (i.e., fine-tune the model on a training set)\n",
    "> - Via model inputs (i.e., insert the knowledge into an input message)\n",
    "> \n",
    "> Although fine-tuning can feel like the more natural option—training on data is how GPT learned all of its other knowledge, after all—we generally do not recommend it as a way to teach the model knowledge. Fine-tuning is better suited to teaching specialized tasks or styles, and is less reliable for factual recall.\n",
    "> \n",
    "> As an analogy, model weights are like long-term memory. When you fine-tune a model, it's like studying for an exam a week away. When the exam arrives, the model may forget details, or misremember facts it never read.\n",
    "> \n",
    "> In contrast, message inputs are like short-term memory. When you insert knowledge into a message, it's like taking an exam with open notes. With notes in hand, the model is more likely to arrive at correct answers.\n",
    "\n",
    "### Retrieval Augmented Generation\n",
    "\n",
    "While we will cover this in more detail in the later chapters, it's worth mentioning here that the system can be combined with retrieved information from a database and then generate a response based on that information. This is called retrieval augmented generation.\n",
    "\n",
    "![](../assets/Retrieval%20Augemented%20Generation.gif)\n",
    "\n",
    "Following are the steps to perform retrieval augmented generation:\n",
    "\n",
    "## Retrieval\n",
    "1. Prepare search data: Prepare a dataset of documents that you want to search through.\n",
    "2. Create embeddings: Create embeddings for each document in the dataset.\n",
    "3. Prepare index: Create an index of the embeddings, this will allow you to search through the documents quickly.\n",
    "4. Search: Search through the documents using the embeddings.\n",
    "\n",
    "## Generation\n",
    "5. Generate: Use the retrieved documents to generate a response.\n",
    "\n",
    "Here, we'll quickly introduce a simplified view of the retrieval using the OpenAI API next:\n",
    "\n",
    "1. Prepare search data: You need to prepare a dataset of documents that you want to search through. This could be a list of documents, a list of paragraphs, or a list of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just loading the data from a text file for illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Path(\"../data/paul_graham/paul_graham_essay.txt\").read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue it's worth considering what would happen if you were to not give this additional context. So the next block of code basically makes a question to the LLM directly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the summer of 2016, Paul Graham likely continued his work as a venture capitalist and co-founder of Y Combinator, a startup accelerator. He may have also been involved in mentoring and advising startups, as well as writing essays and giving talks on entrepreneurship and technology.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask(query: str, model: str = MODEL) -> str:\n",
    "    \"\"\"Return the response to a query.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "ask(\"What did Paul Graham do in Summer of 2016?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Processing: Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of \"GPT\" tokens in your documents: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16534"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_tokens(text: str, model: str = MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "num_tokens(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model can take a maximum of 16,385 which is less than the number of tokens in the document. We need to chunk the document into smaller pieces.\n",
    "\n",
    "Here, we'll simply split the document into approximate chunks of 1024 tokens each. This heuristic is based on empirical experiments [here](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5) by the good folks at LlamaIndex.\n",
    "\n",
    "We'd also recommend using [ChunkWiz](https://chunkviz.up.railway.app/) to build your intuition about the chunking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,\n",
       " 'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=96,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([text])\n",
    "context_text = [t.page_content for t in texts]\n",
    "len(context_text), context_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making embeddings for each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "\n",
    "def create_embeddings(texts: List[str], model: str = EMBEDDING_MODEL) -> List[str]:\n",
    "    \"\"\"Return the embeddings for a list of texts.\"\"\"\n",
    "    return client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "response = create_embeddings(context_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 101\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents: {len(response.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this in an index using Voyager. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 3072)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare a list of embeddigns from the response object\n",
    "\n",
    "vectors = [d.embedding for d in response.data]\n",
    "len(vectors), len(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<voyager.FloatIndex space=Cosine num_dimensions=3072 storage_data_type=Float32>\n"
     ]
    }
   ],
   "source": [
    "from voyager import Index, Space\n",
    "\n",
    "# Create an empty Index object that can store vectors:\n",
    "index = Index(Space.Cosine, num_dimensions=3072)\n",
    "index.add_items(vectors)\n",
    "\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = create_embeddings([\"What did Paul Graham do in Summer of 2016?\"]).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71 62]\n",
      "[0.4730066  0.47339076]\n"
     ]
    }
   ],
   "source": [
    "# # Find the two closest elements:\n",
    "neighbors, distances = index.query(query_vector, k=2)\n",
    "print(neighbors)  # => [0, 1]\n",
    "print(distances)  # => [0.0, 125.0]\n",
    "\n",
    "# # Save the index to disk to reload later (or in Java!)\n",
    "# index.save(\"output_file.voy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I could not find an answer.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_with_context(query: str, context: List[str], model: str = MODEL) -> str:\n",
    "    introduction = \"\"\"Use the below writing from Paul Graham to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"\"\"\n",
    "    question = f\"\\n\\nQuestion: {query}\\n\\n\"\n",
    "    context = \"\\n\\n\".join(context)\n",
    "    return ask(introduction + context + question, model)\n",
    "\n",
    "selected_context = [context_text[neighbors[0]]]\n",
    "ask_with_context(\"What did Paul Graham do in Summer of 2016?\", selected_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the model response was more helpful than the search results. But in many cases, the search results will be more helpful than the model response. We encourage you to \"improve retrieval\" and try different search strategies to see how the model responds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
